{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677737c7-29c3-4b94-aba3-0ddcbd46aec6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Machine-Learning\\anaconda3\\envs\\colab_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n"
     ]
    }
   ],
   "source": [
    "# Import all necessary libraries\n",
    "\n",
    "import gradio as gr\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import tempfile\n",
    "import os\n",
    "import gc\n",
    "import cpuinfo\n",
    "import zipfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "from moviepy.editor import VideoFileClip\n",
    "from matplotlib import animation\n",
    "from sklearn.decomposition import PCA\n",
    "from torchvision import datasets, transforms\n",
    "import shutil\n",
    "from sklearn.utils import shuffle\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "from PIL import Image, ImageFile\n",
    "import logging \n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.animation as animation\n",
    "import subprocess\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# This function finds a safe path to copy to. \n",
    "# Input: path\n",
    "# Output: safe path to write files to\n",
    "def safe_output(path):\n",
    "    if os.path.isdir(path):\n",
    "        gr.Warning(f\"Expected a file, got a directory: {path}\")\n",
    "    return path\n",
    "\n",
    "\n",
    "OUTPUT_DIR = os.path.abspath(\"outputs\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# This line sets the GPU as the device to train on otherwise sets the device to CPU if no GPU found\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------\n",
    "# Layer Functions\n",
    "# --------------------------------\n",
    "\n",
    "\n",
    "# The dictionary containing a map of all layers. Used in wiring the frontend\n",
    "layer_map = {\n",
    "    \"Linear\": lambda in_dim, out_dim: nn.Linear(int(in_dim), int(out_dim)),\n",
    "    \"Conv2d\": lambda in_dim, out_dim: nn.Conv2d(int(in_dim), int(out_dim), kernel_size=3, padding=1),  \n",
    "    \"MaxPool2d\": lambda *_: nn.MaxPool2d(kernel_size=2),  \n",
    "    \"AvgPool2d\": lambda *_: nn.AvgPool2d(kernel_size=2),  \n",
    "    \"Dropout\": lambda p=0.5, *_: nn.Dropout(float(p)),\n",
    "    \"ReLU\": lambda *_: nn.ReLU(),\n",
    "    \"Tanh\": lambda *_: nn.Tanh(),\n",
    "    \"Sigmoid\": lambda *_: nn.Sigmoid(),\n",
    "    \"Flatten\": lambda *_: nn.Flatten(),\n",
    "    \"Softmax\": lambda *_: nn.Softmax(dim=1),\n",
    "    \"LeakyReLU\": lambda slope=0.01, *_: nn.LeakyReLU(negative_slope=float(slope)),\n",
    "    \"GELU\": lambda *_: nn.GELU(),\n",
    "    \"ELU\": lambda alpha=1.0, *_: nn.ELU(alpha=float(alpha))\n",
    "}\n",
    "\n",
    "# This helper function turns integers entered as csv into tuples\n",
    "# Input: An integer, such as 5, or 3,4, or invalid pair such as 3,car\n",
    "# Output: A tuple, such as 5 as an int or (3,4) or a gr.Warning respectively\n",
    "def parse_int_or_tuple(val):\n",
    "    try:\n",
    "        return tuple(map(int, str(val).split(','))) if ',' in str(val) else int(val)\n",
    "    except Exception:\n",
    "        gr.Warning(f\"Invalid numeric input: '{val}'. Please enter an integer or comma-separated pair.\")\n",
    "\n",
    "layer_configs = []\n",
    "\n",
    "# This function checks all dimensions and makes sures correct dimensions are being passed in. \n",
    "# Input: A layer type and dimensions passed in.\n",
    "# Output: Nothing if correct and false otherwise along with a message\n",
    "def validate_layer_inputs(layer_type, **kwargs):\n",
    "    try:\n",
    "        if layer_type == \"Linear\":\n",
    "            in_dim_int = int(kwargs.get(\"in_dim\"))\n",
    "            out_dim_int = int(kwargs.get(\"out_dim\"))\n",
    "            if in_dim_int <= 0 or out_dim_int <= 0:\n",
    "                return False, f\"{layer_type} dimensions must be positive integers\"\n",
    "\n",
    "        elif layer_type == \"Conv2d\":\n",
    "            in_dim_int = int(kwargs.get(\"in_dim\"))\n",
    "            out_dim_int = int(kwargs.get(\"out_dim\"))\n",
    "            kernel_dim = parse_int_or_tuple(kwargs.get(\"kernel_size\", 3))\n",
    "            padding_dim = parse_int_or_tuple(kwargs.get(\"padding\", 1))\n",
    "            stride = parse_int_or_tuple(kwargs.get(\"stride\", 1))\n",
    "\n",
    "            for val in [kernel_dim, padding_dim, stride]:\n",
    "                if isinstance(val, tuple):\n",
    "                    if any(v < 0 for v in val):\n",
    "                        return False, f\"{layer_type} tuple values must be non-negative\"\n",
    "                else:\n",
    "                    if val < 0:\n",
    "                        return False, f\"{layer_type} values must be non-negative\"\n",
    "\n",
    "            if in_dim_int <= 0 or out_dim_int <= 0:\n",
    "                return False, f\"{layer_type} in/out dims must be positive integers\"\n",
    "\n",
    "        elif layer_type == \"Dropout\":\n",
    "            p = float(kwargs.get(\"in_dim\"))\n",
    "            if not (0 <= p <= 1):\n",
    "                return False, \"Dropout probability must be between 0 and 1\"\n",
    "\n",
    "        elif layer_type == \"MaxPool2d\":\n",
    "            kernel = parse_int_or_tuple(kwargs.get(\"pool_kernel\", 2))\n",
    "            stride = parse_int_or_tuple(kwargs.get(\"pool_stride\", 2))\n",
    "            padding = parse_int_or_tuple(kwargs.get(\"pool_padding\", 0))\n",
    "            for val in [kernel, stride, padding]:\n",
    "                if isinstance(val, tuple):\n",
    "                    if any(v < 0 for v in val):\n",
    "                        return False, f\"{layer_type} tuple values must be non-negative\"\n",
    "                else:\n",
    "                    if val < 0:\n",
    "                        return False, f\"{layer_type} values must be non-negative\"\n",
    "        \n",
    "        elif layer_type == \"AvgPool2d\":\n",
    "            kernel = parse_int_or_tuple(kwargs.get(\"avgpool_kernel\", 2))\n",
    "            stride = parse_int_or_tuple(kwargs.get(\"avgpool_stride\", 2))\n",
    "            padding = parse_int_or_tuple(kwargs.get(\"avgpool_padding\", 0))\n",
    "            for val in [kernel, stride, padding]:\n",
    "                if isinstance(val, tuple):\n",
    "                    if any(v < 0 for v in val):\n",
    "                        return False, f\"{layer_type} tuple values must be non-negative\"\n",
    "                else:\n",
    "                    if val < 0:\n",
    "                        return False, f\"{layer_type} values must be non-negative\"\n",
    "\n",
    "        elif layer_type == \"LeakyReLU\":\n",
    "            slope = float(kwargs.get(\"leaky_slope\", 0.01))\n",
    "            if slope < 0:\n",
    "                return False, \"LeakyReLU negative_slope must be ≥ 0\"\n",
    "        \n",
    "        elif layer_type == \"ELU\":\n",
    "            alpha = float(kwargs.get(\"elu_alpha\", 1.0))\n",
    "            if alpha < 0:\n",
    "                return False, \"ELU alpha must be ≥ 0\"\n",
    "\n",
    "        return True, None\n",
    "    except Exception as e:\n",
    "        return False, f\"Validation error in {layer_type}: {str(e)}\"\n",
    "\n",
    "# This function adds layers as the user requests on the frontend\n",
    "# Input: Parameters and type of layer passed in from the frontend\n",
    "# Output: A layer of the type requested is added or an error message is passed to the frontend\n",
    "def add_layer(\n",
    "    layer_type, in_dim, out_dim,\n",
    "    kernel_size=3, padding=1, stride=1, bias=1,\n",
    "    pool_kernel=\"2\", pool_stride=\"2\", pool_padding=\"0\",\n",
    "    avgpool_kernel=None, avgpool_stride=None, avgpool_padding=None,\n",
    "    leaky_slope = \"0.01\", elu_alpha = \"1.0\"\n",
    "):\n",
    "\n",
    "    is_valid, err_msg = validate_layer_inputs(\n",
    "        layer_type=layer_type,\n",
    "        in_dim=in_dim,\n",
    "        out_dim=out_dim,\n",
    "        kernel_size=kernel_size,\n",
    "        padding=padding,\n",
    "        stride=stride,\n",
    "        pool_kernel=pool_kernel,\n",
    "        pool_stride=pool_stride,\n",
    "        pool_padding=pool_padding,\n",
    "        avgpool_kernel=avgpool_kernel, \n",
    "        avgpool_stride=avgpool_stride,\n",
    "        avgpool_padding=avgpool_padding,\n",
    "        leaky_slope=leaky_slope,\n",
    "        elu_alpha=elu_alpha\n",
    "    )\n",
    "\n",
    "    if not is_valid:\n",
    "        return err_msg\n",
    "\n",
    "    try:\n",
    "        if layer_type == \"Conv2d\":\n",
    "            in_dim = int(in_dim)\n",
    "            out_dim = int(out_dim)\n",
    "            k = parse_int_or_tuple(kernel_size or 3)\n",
    "            p = parse_int_or_tuple(padding or 1)\n",
    "            s = parse_int_or_tuple(stride or 1)\n",
    "            b = bool(bias)\n",
    "            desc = f\"Conv2d({in_dim}, {out_dim}, kernel={k}, padding={p}, stride={s}, bias={b})\"\n",
    "            config = (desc, layer_type, in_dim, out_dim, k, p, s, b)\n",
    "        \n",
    "        elif layer_type == \"LeakyReLU\":\n",
    "            negative_slope = float(leaky_slope or \"0.01\")\n",
    "            desc = f\"LeakyReLU(negative_slope={negative_slope})\"\n",
    "            config = (desc, layer_type, negative_slope, negative_slope, None, None, None, None)\n",
    "        \n",
    "        elif layer_type == \"ELU\":\n",
    "            alpha = float(elu_alpha or \"1.0\")\n",
    "            desc = f\"ELU(alpha={alpha})\"\n",
    "            config = (desc, layer_type, alpha, alpha, None, None, None, None)\n",
    "\n",
    "        elif layer_type == \"Softmax\":\n",
    "            desc = \"Softmax(dim=1)\"\n",
    "            config = (desc, layer_type, None, None, None, None, None, None)\n",
    "\n",
    "        elif layer_type == \"GELU\":\n",
    "            desc = \"GELU()\"\n",
    "            config = (desc, layer_type, None, None, None, None, None, None)\n",
    "            \n",
    "        elif layer_type == \"Linear\":\n",
    "            in_dim = int(in_dim)\n",
    "            out_dim = int(out_dim)\n",
    "            desc = f\"Linear({in_dim}, {out_dim})\"\n",
    "            config = (desc, layer_type, in_dim, out_dim, None, None, None, None)\n",
    "\n",
    "        elif layer_type == \"Dropout\":\n",
    "            p = float(in_dim)\n",
    "            desc = f\"Dropout({p})\"\n",
    "            config = (desc, layer_type, p, p, None, None, None, None)\n",
    "\n",
    "        elif layer_type == \"MaxPool2d\":\n",
    "            kernel_val = parse_int_or_tuple(pool_kernel or \"2\")\n",
    "            stride_val = parse_int_or_tuple(pool_stride or \"2\")\n",
    "            padding_val = parse_int_or_tuple(pool_padding or \"0\")\n",
    "            desc = f\"MaxPool2d(kernel={kernel_val}, stride={stride_val}, padding={padding_val})\"\n",
    "            config = (desc, layer_type, None, None, kernel_val, padding_val, stride_val, None)\n",
    "\n",
    "        elif layer_type == \"AvgPool2d\":\n",
    "            kernel_val = parse_int_or_tuple(avgpool_kernel or \"2\")\n",
    "            stride_val = parse_int_or_tuple(avgpool_stride or \"2\")\n",
    "            padding_val = parse_int_or_tuple(avgpool_padding or \"0\")\n",
    "            desc = f\"AvgPool2d(kernel={kernel_val}, stride={stride_val}, padding={padding_val})\"\n",
    "            config = (desc, layer_type, None, None, kernel_val, padding_val, stride_val, None)\n",
    "\n",
    "        else:\n",
    "            # For e.g. ReLU/Tanh/Sigmoid/Flatten\n",
    "            desc = layer_type\n",
    "            config = (desc, layer_type, None, None, None, None, None, None)\n",
    "            \n",
    "    except Exception as e:\n",
    "        desc = f\"[Error Adding Layer: {e}]\"\n",
    "        config = (desc, layer_type, None, None, None, None, None, None)\n",
    "\n",
    "    layer_configs.append(config)\n",
    "    return update_architecture_text()\n",
    "\n",
    "# This function updates layers as the user requests on the frontend\n",
    "# Input: Parameters and type of layer passed in from the frontend\n",
    "# Output: A layer of the type requested is added or an error message is passed to the frontend\n",
    "def update_layer(\n",
    "    index, layer_type, in_dim, out_dim,\n",
    "    kernel_size=3, padding=1, stride=1, bias=True,\n",
    "    pool_kernel=\"2\", pool_stride=\"2\", pool_padding=\"0\",\n",
    "    avgpool_kernel=None, avgpool_stride=None, avgpool_padding=None,\n",
    "    leaky_slope= \"0.01\",elu_alpha = \"1.0\"\n",
    "):\n",
    "    \n",
    "    index = int(index)\n",
    "    is_valid, err_msg = validate_layer_inputs(\n",
    "        layer_type=layer_type,\n",
    "        in_dim=in_dim,\n",
    "        out_dim=out_dim,\n",
    "        kernel_size=kernel_size,\n",
    "        padding=padding,\n",
    "        stride=stride,\n",
    "        pool_kernel=pool_kernel,\n",
    "        pool_stride=pool_stride,\n",
    "        pool_padding=pool_padding,\n",
    "        avgpool_kernel=avgpool_kernel, \n",
    "        avgpool_stride=avgpool_stride,\n",
    "        avgpool_padding=avgpool_padding,\n",
    "        leaky_slope=leaky_slope,\n",
    "        elu_alpha=elu_alpha\n",
    "    )\n",
    "\n",
    "    if not is_valid:\n",
    "        return err_msg\n",
    "    if index < 0 or index >= len(layer_configs):\n",
    "        return update_architecture_text()\n",
    "\n",
    "    try:\n",
    "        if layer_type == \"Conv2d\":\n",
    "            i = int(in_dim)\n",
    "            o = int(out_dim)\n",
    "            k = parse_int_or_tuple(kernel_size or 3)\n",
    "            p = parse_int_or_tuple(padding or 1)\n",
    "            s = parse_int_or_tuple(stride or 1)\n",
    "            b = bool(bias)\n",
    "            desc = f\"Conv2d({i}, {o}, kernel={k}, padding={p}, stride={s}, bias={b})\"\n",
    "            layer_configs[index] = (desc, layer_type, i, o, k, p, s, b)\n",
    "\n",
    "        elif layer_type == \"Linear\":\n",
    "            i = int(in_dim)\n",
    "            o = int(out_dim)\n",
    "            desc = f\"Linear({i}, {o})\"\n",
    "            layer_configs[index] = (desc, layer_type, i, o, None, None, None, None)\n",
    "        \n",
    "        elif layer_type == \"ELU\":\n",
    "            alpha = float(elu_alpha or \"1.0\")\n",
    "            desc = f\"ELU(alpha={alpha})\"\n",
    "            layer_configs[index] = (desc, layer_type, alpha, alpha, None, None, None, None)\n",
    "\n",
    "        elif layer_type == \"GELU\":\n",
    "            desc = \"GELU()\"\n",
    "            layer_configs[index] = (desc, layer_type, None, None, None, None, None, None)\n",
    "\n",
    "        elif layer_type == \"LeakyReLU\":\n",
    "            negative_slope = float(leaky_slope or \"0.01\")\n",
    "            desc = f\"LeakyReLU(negative_slope={negative_slope})\"\n",
    "            layer_configs[index] = (desc, layer_type, negative_slope, negative_slope, None, None, None, None)\n",
    "\n",
    "        elif layer_type == \"Softmax\":\n",
    "            desc = \"Softmax(dim=1)\"\n",
    "            layer_configs[index] = (desc, layer_type, None, None, None, None, None, None)\n",
    "\n",
    "        elif layer_type == \"Dropout\":\n",
    "            p = float(in_dim)\n",
    "            desc = f\"Dropout({p})\"\n",
    "            layer_configs[index] = (desc, layer_type, p, p, None, None, None, None)\n",
    "\n",
    "        elif layer_type == \"AvgPool2d\":\n",
    "            kv = parse_int_or_tuple(avgpool_kernel or \"2\")\n",
    "            sv = parse_int_or_tuple(avgpool_stride or \"2\")\n",
    "            pv = parse_int_or_tuple(avgpool_padding or \"0\")\n",
    "            desc = f\"AvgPool2d(kernel={kv}, stride={sv}, padding={pv})\"\n",
    "            layer_configs[index] = (desc, layer_type, None, None, kv, pv, sv, None)\n",
    "\n",
    "        elif layer_type == \"MaxPool2d\":\n",
    "            kv = parse_int_or_tuple(pool_kernel or \"2\")\n",
    "            sv = parse_int_or_tuple(pool_stride or \"2\")\n",
    "            pv = parse_int_or_tuple(pool_padding or \"0\")\n",
    "            desc = f\"MaxPool2d(kernel={kv}, stride={sv}, padding={pv})\"\n",
    "            layer_configs[index] = (desc, layer_type, None, None, kv, pv, sv, None)\n",
    "\n",
    "        else:\n",
    "            desc = layer_type\n",
    "            layer_configs[index] = (desc, layer_type, None, None, None, None, None, None)\n",
    "\n",
    "    except Exception as e:\n",
    "        layer_configs[index] = (f\"[Error Editing Layer: {e}]\", layer_type, None, None, None, None, None, None)\n",
    "\n",
    "    return update_architecture_text()\n",
    "\n",
    "# This function inserts layers as the user requests on the frontend\n",
    "# Input: Parameters and type of layer passed in from the frontend\n",
    "# Output: A layer of the type requested is added or an error message is passed to the frontend\n",
    "def insert_layer(\n",
    "    index, layer_type, in_dim, out_dim,\n",
    "    kernel_size=3, padding=1, stride=1, bias=1,\n",
    "    pool_kernel=\"2\", pool_stride=\"2\", pool_padding=\"0\",\n",
    "    avgpool_kernel=None, avgpool_stride=None, avgpool_padding=None, \n",
    "    leaky_slope=\"0.01\", elu_alpha = \"1.0\"\n",
    "):\n",
    "    index = int(index)\n",
    "    is_valid, err_msg = validate_layer_inputs(\n",
    "        layer_type=layer_type,\n",
    "        in_dim=in_dim,\n",
    "        out_dim=out_dim,\n",
    "        kernel_size=kernel_size,\n",
    "        padding=padding,\n",
    "        stride=stride,\n",
    "        pool_kernel=pool_kernel,\n",
    "        pool_stride=pool_stride,\n",
    "        pool_padding=pool_padding,\n",
    "        avgpool_kernel=avgpool_kernel, \n",
    "        avgpool_stride=avgpool_stride,\n",
    "        avgpool_padding=avgpool_padding,\n",
    "        leaky_slope=leaky_slope,\n",
    "        elu_alpha=elu_alpha\n",
    "    )\n",
    "\n",
    "    if not is_valid:\n",
    "        return err_msg\n",
    "\n",
    "    try:\n",
    "        if layer_type == \"Conv2d\":\n",
    "            i = int(in_dim)\n",
    "            o = int(out_dim)\n",
    "            k = parse_int_or_tuple(kernel_size or \"3\")\n",
    "            p = parse_int_or_tuple(padding or \"1\")\n",
    "            s = parse_int_or_tuple(stride or \"1\")\n",
    "            b = bool(bias)\n",
    "            desc = f\"Conv2d({i}, {o}, kernel={k}, padding={p}, stride={s}, bias={b})\"\n",
    "            layer_configs.insert(index, (desc, layer_type, i, o, k, p, s, b))\n",
    "\n",
    "        elif layer_type == \"Linear\":\n",
    "            i = int(in_dim)\n",
    "            o = int(out_dim)\n",
    "            desc = f\"Linear({i}, {o})\"\n",
    "            layer_configs.insert(index, (desc, layer_type, i, o, None, None, None, None))\n",
    "\n",
    "        elif layer_type == \"ELU\":\n",
    "            alpha = float(elu_alpha or \"1.0\")\n",
    "            desc = f\"ELU(alpha={alpha})\"\n",
    "            layer_configs.insert(index, (desc, layer_type, alpha, alpha, None, None, None, None))\n",
    "\n",
    "        elif layer_type == \"LeakyReLU\":\n",
    "            negative_slope = float(leaky_slope or \"0.01\")\n",
    "            desc = f\"LeakyReLU(negative_slope={negative_slope})\"\n",
    "            layer_configs.insert(index, (desc, layer_type, negative_slope, negative_slope, None, None, None, None))\n",
    "\n",
    "        elif layer_type == \"Softmax\":\n",
    "            desc = \"Softmax(dim=1)\"\n",
    "            layer_configs.insert(index, (desc, layer_type, None, None, None, None, None, None))\n",
    "\n",
    "        elif layer_type == \"Dropout\":\n",
    "            p = float(in_dim)\n",
    "            desc = f\"Dropout({p})\"\n",
    "            layer_configs.insert(index, (desc, layer_type, p, p, None, None, None, None))\n",
    "\n",
    "        elif layer_type == \"MaxPool2d\":\n",
    "            kv = parse_int_or_tuple(pool_kernel or \"2\")\n",
    "            sv = parse_int_or_tuple(pool_stride or \"2\")\n",
    "            pv = parse_int_or_tuple(pool_padding or \"0\")\n",
    "            desc = f\"MaxPool2d(kernel={kv}, stride={sv}, padding={pv})\"\n",
    "            layer_configs.insert(index, (desc, layer_type, None, None, kv, pv, sv, None))\n",
    "\n",
    "        elif layer_type == \"AvgPool2d\":\n",
    "            kv = parse_int_or_tuple(avgpool_kernel or \"2\")\n",
    "            sv = parse_int_or_tuple(avgpool_stride or \"2\")\n",
    "            pv = parse_int_or_tuple(avgpool_padding or \"0\")\n",
    "            desc = f\"AvgPool2d(kernel={kv}, stride={sv}, padding={pv})\"\n",
    "            layer_configs.insert(index, (desc, layer_type, None, None, kv, pv, sv, None))\n",
    "\n",
    "        elif layer_type == \"GELU\":\n",
    "            desc = \"GELU()\"\n",
    "            layer_configs.insert(index, (desc, layer_type, None, None, None, None, None, None))\n",
    "\n",
    "        else:\n",
    "            desc = layer_type\n",
    "            layer_configs.insert(index, (desc, layer_type, None, None, None, None, None, None))\n",
    "\n",
    "    except Exception as e:\n",
    "        desc = f\"[Error Inserting Layer: {e}]\"\n",
    "        layer_configs.insert(index, (desc, layer_type, None, None, None, None, None, None))\n",
    "\n",
    "    return update_architecture_text()\n",
    "\n",
    "# This function deletes layers as the user requests on the frontend in the edit tab\n",
    "def delete_layer(index):\n",
    "    index = int(index)\n",
    "    if 0 <= index < len(layer_configs):\n",
    "        layer_configs.pop(index)\n",
    "    return update_architecture_text()\n",
    "    \n",
    "# This function clears all layers currently selected if the user hits reset on the frontend\n",
    "def reset_layers():\n",
    "    layer_configs.clear()\n",
    "    return \"\"\n",
    "\n",
    "# This helper function shows the error messages if dimensions do not match. It does not calculate them it is simply called to show the visual error message. \n",
    "# Input: index to highlight at\n",
    "# Output: A warning printed on the architecture at the mismatch layer\n",
    "def update_architecture_text(highlight_index=None):\n",
    "    lines = []\n",
    "    for i, config in enumerate(layer_configs):\n",
    "        prefix = f\"{i}: \"\n",
    "        desc = config[0]\n",
    "        if i == highlight_index:\n",
    "            desc = f\"⚠️ {desc}\"\n",
    "        lines.append(prefix + desc)\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# This function builds the model the user requests\n",
    "# Input: Nothing\n",
    "# Output: The model built and passed into training\n",
    "def build_model():\n",
    "    layers = []\n",
    "    for config in layer_configs:\n",
    "        _, layer_type, in_dim, out_dim, kernel, padding, stride, bias = config\n",
    "\n",
    "        if layer_type == \"Conv2d\":\n",
    "            layers.append(nn.Conv2d(\n",
    "                int(in_dim), int(out_dim),\n",
    "                kernel_size=kernel,\n",
    "                padding=padding,\n",
    "                stride=stride,\n",
    "                bias=bool(bias)\n",
    "            ))\n",
    "\n",
    "        elif layer_type == \"Linear\":\n",
    "            layers.append(nn.Linear(int(in_dim), int(out_dim)))\n",
    "        \n",
    "        elif layer_type == \"GELU\":\n",
    "            layers.append(nn.GELU())\n",
    "\n",
    "        elif layer_type == \"LeakyReLU\":\n",
    "            slope = in_dim if in_dim is not None else 0.01  \n",
    "            layers.append(nn.LeakyReLU(negative_slope=float(slope)))\n",
    "\n",
    "        elif layer_type == \"ELU\":\n",
    "            alpha = in_dim if in_dim is not None else 1.0  \n",
    "            layers.append(nn.ELU(alpha=float(alpha)))\n",
    "\n",
    "        elif layer_type == \"Dropout\":\n",
    "            p = in_dim if in_dim is not None else 0.5  \n",
    "            layers.append(nn.Dropout(float(p)))\n",
    "        \n",
    "        elif layer_type == \"Softmax\":\n",
    "            layers.append(nn.Softmax(dim=1))\n",
    "\n",
    "        elif layer_type == \"MaxPool2d\":\n",
    "            layers.append(nn.MaxPool2d(kernel_size=kernel, stride=stride, padding=padding))\n",
    "\n",
    "        elif layer_type == \"AvgPool2d\":\n",
    "            layers.append(nn.AvgPool2d(kernel_size=kernel, stride=stride, padding=padding))\n",
    "\n",
    "        else:\n",
    "            # ReLU, Tanh, Sigmoid, Flatten\n",
    "            layers.append(layer_map[layer_type]())\n",
    "\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "# --------------------------------\n",
    "# Data Loaders\n",
    "# --------------------------------\n",
    "\n",
    "# This helper function extracts the zip file provided for images to a custom path as the user requests on the frontend or into a C: drive temp file\n",
    "# Input: File and custom path\n",
    "# Output: The unzipped file st the custom path or at the C: drive\n",
    "def extract_zip_to_tempdir(file_like, custom_path=None):\n",
    "\n",
    "    if custom_path:\n",
    "        os.makedirs(custom_path, exist_ok=True)\n",
    "        temp_dir = tempfile.mkdtemp(dir=custom_path)\n",
    "    else:\n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "    with zipfile.ZipFile(file_like, 'r') as zip_ref:\n",
    "        zip_ref.extractall(temp_dir)\n",
    "    return temp_dir\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# This helper function checks if any images are corrupted. To prevent a crash it skips them and makes a info bubble. \n",
    "# Input: Path and number of channels of the images\n",
    "# Output: Info bubble if corrupt image\n",
    "def safe_pil_loader(path, num_channels=3):\n",
    "    try:\n",
    "        with open(path, 'rb') as f:\n",
    "            img = Image.open(f)\n",
    "            img = img.convert('RGB' if num_channels == 3 else 'L')\n",
    "            img.load()\n",
    "            return img\n",
    "    except Exception as e:\n",
    "        gr.Info(f\"Skipping corrupted image: {path} — {e}\")\n",
    "        return None\n",
    "\n",
    "# This class safely loads images from a folder, skipping invalid or unreadable files.\n",
    "# Input: root (dataset folder path), transform (optional image transforms), num_channels (number of image channels)\n",
    "# Output: Filters out invalid images during dataset initialization, ensuring only valid samples are used.\n",
    "class SafeImageFolder(datasets.ImageFolder):\n",
    "    def __init__(self, root, transform=None, num_channels=3):\n",
    "        loader_with_channels = lambda path: safe_pil_loader(path, num_channels)\n",
    "        super().__init__(root, transform=transform, loader=loader_with_channels)\n",
    "        \n",
    "        before = len(self.samples)\n",
    "        self.samples = [s for s in self.samples if loader_with_channels(s[0]) is not None]\n",
    "        after = len(self.samples)\n",
    "        self.imgs = self.samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        img = self.loader(path)\n",
    "        if img is None:\n",
    "            gr.Warning(f\"Could not load image at {path}\")\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, target\n",
    "\n",
    "# This function loads data. It calls the helper functions and loads clean, uncorrupted data. \n",
    "# Input: file, custom_path=None, batch_size=32, image_size=28,  num_channels=3, loss_fn=None\n",
    "# Output: Clean data ready for training\n",
    "def load_data(file, custom_path=None, batch_size=32, image_size=28,  num_channels=3, loss_fn=None):\n",
    "    ext = os.path.splitext(file)[1].lower()\n",
    "    if ext == \".csv\":\n",
    "\n",
    "        df = pd.read_csv(file)\n",
    "        if 'y' not in df.columns:\n",
    "            gr.Warning(\"CSV must contain a 'y' column.\")\n",
    "        \n",
    "        X = df.drop(columns=['y']).values.astype(np.float32)\n",
    "        if isinstance(loss_fn, nn.CrossEntropyLoss):\n",
    "            y = df['y'].values.astype(np.int64)\n",
    "        elif isinstance(loss_fn, (nn.MSELoss, nn.BCEWithLogitsLoss)):\n",
    "            y = df['y'].values.astype(np.float32)\n",
    "        else:\n",
    "            gr.Warning(f\"Unhandled loss function type: {type(loss_fn)}\")\n",
    "    \n",
    "        X, y = shuffle(X, y)\n",
    "\n",
    "        return {\n",
    "            \"type\": \"tabular\",\n",
    "            \"train\": (X, y),\n",
    "            \"path\": None\n",
    "        }\n",
    "\n",
    "    elif ext == \".zip\":\n",
    "        with open(file, 'rb') as f:\n",
    "            data_dir = extract_zip_to_tempdir(f, custom_path)\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        dataset = SafeImageFolder(data_dir, transform=transform, num_channels=num_channels)\n",
    "        train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        return {\n",
    "            \"type\": \"image\",\n",
    "            \"train\": train_loader,\n",
    "            \"path\": data_dir\n",
    "        }\n",
    "    else:\n",
    "        gr.Warning(\"Unsupported file format. Provide a .csv or .zip path.\")\n",
    "\n",
    "\n",
    "# --------------------------------\n",
    "# Graph Functions\n",
    "# --------------------------------\n",
    "\n",
    "\n",
    "# This function flattens the model parameters into a vector which then appears as a ball on the 3d plot. \n",
    "# Input: model\n",
    "# Output: flattened parameters\n",
    "def get_flat_weights(model):\n",
    "    return torch.cat([p.detach().flatten() for p in model.parameters()])\n",
    "\n",
    "# This function plots the 2d graph showing the loss history of the model.\n",
    "# Input: Loss_history\n",
    "# Output: A 2d graph showing the loss history per epoch\n",
    "def generate_loss_plot(loss_history):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(loss_history)\n",
    "    ax.set_title(\"Loss over Epochs\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    tmpfile = tempfile.NamedTemporaryFile(suffix=\".png\", delete=False)\n",
    "    plt.savefig(tmpfile.name)\n",
    "    tmpfile.close()\n",
    "    plt.close(fig)\n",
    "    return tmpfile.name\n",
    "\n",
    "# This function generates the 3d video of teh loss\n",
    "# Input: weight_path, loss_history, output_path, target_frames=300, frame_rate=10\n",
    "# Output: A video showing the function descending to a minima or a warning\n",
    "def generate_3d_animation_pca(weight_path, loss_history, output_path, target_frames=300, frame_rate=10):\n",
    "\n",
    "    if len(loss_history) == 0 or np.any(np.isnan(loss_history)):\n",
    "        gr.Warning(\"❌ loss_history is empty or contains NaNs — cannot animate.\")\n",
    "        return\n",
    "\n",
    "    if len(weight_path) < target_frames:\n",
    "        target_frames = len(weight_path)\n",
    "\n",
    "\n",
    "    indices = np.linspace(0, len(weight_path) - 1, target_frames).astype(int)\n",
    "    weight_path_sampled = weight_path[indices]\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced = pca.fit_transform(weight_path_sampled)\n",
    "\n",
    "    interpolated_loss = np.interp(\n",
    "        np.linspace(0, len(loss_history) - 1, target_frames),\n",
    "        np.arange(len(loss_history)),\n",
    "        loss_history\n",
    "    )\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.set_title(\"Training Progression\")\n",
    "    ax.set_xlabel(\"PCA 1\")\n",
    "    ax.set_ylabel(\"PCA 2\")\n",
    "    ax.set_zlabel(\"Loss\")\n",
    "\n",
    "    ax.plot(reduced[:, 0], reduced[:, 1], interpolated_loss, color='red', alpha=0.6)\n",
    "    point, = ax.plot([reduced[0, 0]], [reduced[0, 1]], [interpolated_loss[0]], 'ro')\n",
    "\n",
    "    def update(i):\n",
    "        point.set_data([reduced[i, 0]], [reduced[i, 1]])\n",
    "        point.set_3d_properties([interpolated_loss[i]])\n",
    "        return point,\n",
    "\n",
    "    interval = 1000 / frame_rate  \n",
    "    ani = animation.FuncAnimation(\n",
    "        fig, update,\n",
    "        frames=target_frames,\n",
    "        interval=interval,\n",
    "        blit=True\n",
    "    )\n",
    "\n",
    "    ani.save(output_path, writer='ffmpeg', fps=frame_rate, codec='libx264')\n",
    "    plt.close(fig)\n",
    "\n",
    "    return output_path\n",
    "\n",
    "# --------------------------------\n",
    "# Training Functions\n",
    "# --------------------------------\n",
    "\n",
    "# This function makes sure that the model being built is a valid model before training\n",
    "# Input: model, data_type, image_size=28, num_features=None, num_channels=3\n",
    "# Output: errors if the model is not valid and a three tuple if the model is valid\n",
    "def validate_model_forward_pass(model, data_type, image_size=28, num_features=None, num_channels=3):\n",
    "    try:\n",
    "        if data_type == \"tabular\":\n",
    "            assert num_features is not None, \"Missing number of input features for tabular data\"\n",
    "            dummy_input = torch.randn(1, num_features).to(device)\n",
    "        else:\n",
    "            dummy_input = torch.randn(1, num_channels, image_size, image_size).to(device)\n",
    "\n",
    "        x = dummy_input\n",
    "        for idx, layer in enumerate(model):\n",
    "            try:\n",
    "                x = layer(x)\n",
    "            except Exception as e:\n",
    "                return False, f\"Shape mismatch at layer {idx}: {layer.__class__.__name__} — {str(e)}\", idx\n",
    "        return True, None, None\n",
    "    except Exception as e:\n",
    "        return False, f\"Unexpected validation error: {str(e)}\", None\n",
    "\n",
    "# This function validates the entire pipeline through a series of tests\n",
    "# Input: X_tensor, y_tensor, model, loss_fn, batch_size=None, auto_fix=True\n",
    "# Output: A gr.Warning if any of the tests is failed and X_tensor, y_tensor which are potentially modified tensors after validation and auto-fixes\n",
    "def full_pipeline_validator(X_tensor, y_tensor, model, loss_fn, batch_size=None, auto_fix=True):\n",
    "\n",
    "    if X_tensor.shape[0] == 0 or y_tensor.shape[0] == 0:\n",
    "        gr.Warning(\"❌ Dataset is empty. Check your data loading pipeline.\")\n",
    "\n",
    "\n",
    "    if torch.isnan(X_tensor).any() or torch.isnan(y_tensor).any():\n",
    "        gr.Warning(\"❌ NaN detected in input or target tensor.\")\n",
    "    if torch.isinf(X_tensor).any() or torch.isinf(y_tensor).any():\n",
    "        gr.Warning(\"❌ Inf detected in input or target tensor.\")\n",
    "    \n",
    "    gr.Info(\"✅ No NaNs or Infs detected.\")\n",
    "\n",
    "\n",
    "    if X_tensor.dtype != torch.float32:\n",
    "        if auto_fix:\n",
    "            gr.Info(f\"⚠️ Auto-fixing X_tensor dtype from {X_tensor.dtype} to torch.float32\")\n",
    "            X_tensor = X_tensor.float()\n",
    "        else:\n",
    "            gr.Warning(f\"❌ X_tensor dtype must be float32, got {X_tensor.dtype}\")\n",
    "\n",
    "    if isinstance(loss_fn, nn.CrossEntropyLoss):\n",
    "        if y_tensor.dtype != torch.long:\n",
    "            if auto_fix:\n",
    "                gr.Info(f\"⚠️ Auto-fixing y_tensor dtype from {y_tensor.dtype} to torch.long\")\n",
    "                y_tensor = y_tensor.long()\n",
    "            else:\n",
    "                gr.Warning(f\"❌ For CrossEntropyLoss, y_tensor dtype must be torch.long, got {y_tensor.dtype}\")\n",
    "    else:\n",
    "        if y_tensor.dtype != torch.float32:\n",
    "            if auto_fix:\n",
    "                gr.Info(f\"⚠️ Auto-fixing y_tensor dtype from {y_tensor.dtype} to torch.float32\")\n",
    "                y_tensor = y_tensor.float()\n",
    "            else:\n",
    "                gr.Warning(f\"❌ y_tensor dtype must be float32, got {y_tensor.dtype}\")\n",
    "\n",
    "    if len(X_tensor.shape) != 2:\n",
    "        gr.Warning(f\"❌ X_tensor must be 2D [batch_size, features], got {X_tensor.shape}\")\n",
    "\n",
    "    if isinstance(loss_fn, (nn.MSELoss, nn.BCEWithLogitsLoss)):\n",
    "        expected_y_shape = (X_tensor.shape[0], 1)\n",
    "        if y_tensor.shape != expected_y_shape:\n",
    "            if auto_fix:\n",
    "                gr.Info(f\"⚠️ Auto-reshaping y_tensor from {y_tensor.shape} to {expected_y_shape}\")\n",
    "                y_tensor = y_tensor.view(-1, 1)\n",
    "            else:\n",
    "                gr.Warning(f\"❌ y_tensor should have shape {expected_y_shape}, but got {y_tensor.shape}\")\n",
    "\n",
    "    elif isinstance(loss_fn, nn.CrossEntropyLoss):\n",
    "        expected_y_shape = (X_tensor.shape[0],)\n",
    "        if y_tensor.shape != expected_y_shape:\n",
    "            if auto_fix:\n",
    "                gr.Info(f\"⚠️ Auto-reshaping y_tensor from {y_tensor.shape} to {expected_y_shape}\")\n",
    "                y_tensor = y_tensor.view(-1)\n",
    "            else:\n",
    "                gr.Warning(f\"❌ y_tensor should have shape {expected_y_shape}, but got {y_tensor.shape}\")\n",
    "\n",
    "    if batch_size is not None and X_tensor.shape[0] < batch_size:\n",
    "        gr.Info(f\"⚠️ Batch size {batch_size} is larger than dataset size {X_tensor.shape[0]}, adjusting.\")\n",
    "        batch_size = X_tensor.shape[0]\n",
    "\n",
    "    try:\n",
    "        model.eval()  \n",
    "        with torch.no_grad():\n",
    "            dummy_out = model(X_tensor[:1])\n",
    "            if isinstance(loss_fn, nn.CrossEntropyLoss):\n",
    "                if dummy_out.ndim != 2:\n",
    "                    gr.Warning(f\"❌ Model output for CrossEntropyLoss should be 2D [batch_size, num_classes], got {dummy_out.shape}\")\n",
    "            else:\n",
    "                if dummy_out.shape != y_tensor[:1].shape:\n",
    "                    gr.Warning(f\"❌ Model output shape {dummy_out.shape} does not match target shape {y_tensor[:1].shape}\")\n",
    "    except Exception as e:\n",
    "        gr.Warning(f\"❌ Model forward pass failed: {e}\")\n",
    "\n",
    " \n",
    "    return X_tensor, y_tensor\n",
    "\n",
    "# This function gets the device status shown on the frontend. \n",
    "# Input: Nothing\n",
    "# Output: The device used and any potential cuda errors\n",
    "def get_device_status():\n",
    "    if not torch.cuda.is_available():\n",
    "        return f\"🔴 CPU: {cpuinfo.get_cpu_info()}\"\n",
    "    try:\n",
    "        device_name = torch.cuda.get_device_name(0)\n",
    "        mem_allocated = torch.cuda.memory_allocated(0) / (1024 ** 2)  \n",
    "        mem_reserved = torch.cuda.memory_reserved(0) / (1024 ** 2)  \n",
    "        total_mem = torch.cuda.get_device_properties(0).total_memory / (1024 ** 2)  \n",
    "        mem_free = total_mem - mem_reserved\n",
    "\n",
    "        try:\n",
    "            torch.cuda.synchronize()\n",
    "            sync_status = \"✅ GPU context healthy.\"\n",
    "        except Exception as sync_error:\n",
    "            sync_status = f\"❌ GPU context error: {sync_error}\"\n",
    "\n",
    "        return (\n",
    "            f\"✅ CUDA Device: {device_name}<br>\"\n",
    "            f\"Memory allocated: {mem_allocated:.2f} MB<br>\"\n",
    "            f\"Memory reserved: {mem_reserved:.2f} MB<br>\"\n",
    "            f\"Free memory (estimated): {mem_free:.2f} MB<br>\"\n",
    "            f\"{sync_status}\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"❌ CUDA error: {str(e)}\"\n",
    "\n",
    "# This is the function that trains the model. It calls many helper functions also and checks many parameters the user passes in from the frontend\n",
    "# Input: loss_name, opt_name, lr, batch_size='32', image_size='28', file=None, custom_path=None, epochs='100', num_channels=3, generate_animation=False,  target_frames=None, frame_rate=None\n",
    "# Output: Warnings, Info and inputs to graph functions and model for user to download. It will also delete the folder that was created to unzip files into at the end of training \n",
    "def train_model(loss_name, opt_name, lr, batch_size='32', image_size='28', file=None, custom_path=None, epochs='100', num_channels=3, generate_animation=False,  target_frames=None, frame_rate=None):\n",
    "\n",
    "    animation_path = None\n",
    "    loss_plot_path = None\n",
    "    model_path = None\n",
    "    final_logs = \"\"\n",
    "    try: \n",
    "        target_frames = int(target_frames)\n",
    "        if target_frames<=0:\n",
    "            yield None, None, None, update_architecture_text(), \"❌ Target frames must be a positive integer.\"\n",
    "            return\n",
    "    except:\n",
    "        yield None, None, None, update_architecture_text(), \"❌ Target frames must be numeric.\"\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        frame_rate = int(frame_rate)\n",
    "        if frame_rate<=0:\n",
    "            yield None, None, None, update_architecture_text(), \"❌ Frame Rate must be a positive integer.\"\n",
    "            return\n",
    "    except:\n",
    "        yield None, None, None, update_architecture_text(), \"❌ Frame Rate must be numeric.\"\n",
    "        return\n",
    "            \n",
    "            \n",
    "    try:\n",
    "        channels = int(num_channels)\n",
    "        if channels not in [1, 3]:\n",
    "            yield None, None, None, update_architecture_text(), \"❌ Channels must be 1 or 3.\"\n",
    "            return\n",
    "    except:\n",
    "        yield None, None, None, update_architecture_text(), \"❌ Channels must be numeric.\"\n",
    "        return\n",
    "\n",
    "    # 1) Convert epochs to int with early validation\n",
    "    try:\n",
    "        max_epochs = int(epochs)\n",
    "        if max_epochs <= 0:\n",
    "            # Immediately yield error and stop\n",
    "            yield None, None, None, update_architecture_text(), \"❌ Epochs must be a positive integer.\"\n",
    "            return\n",
    "    except:\n",
    "        yield None, None, None, update_architecture_text(), \"❌ Epochs must be a valid number.\"\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # 2) If no model is configured\n",
    "        if not layer_configs:\n",
    "            yield None, None, None, update_architecture_text(), \"❌ No model configured! Please add at least one trainable layer.\"\n",
    "            return\n",
    "\n",
    "        # 3) If file path is invalid\n",
    "        if not os.path.exists(file):\n",
    "            msg = f\"❌ File not found: {file}\\nPlease check the path and try again.\"\n",
    "            yield None, None, None, update_architecture_text(), msg\n",
    "            return\n",
    "\n",
    "        # 4) If file type is not .csv or .zip\n",
    "        if not (file.endswith('.csv') or file.endswith('.zip')):\n",
    "            yield None, None, None, update_architecture_text(), \"❌ Invalid file type. Please provide a .csv or .zip file.\"\n",
    "            return\n",
    "\n",
    "        # 5) If custom directory is invalid\n",
    "        if custom_path and not os.path.isdir(custom_path):\n",
    "            try:\n",
    "                os.makedirs(custom_path, exist_ok=True)\n",
    "            except Exception as e:\n",
    "                msg = f\"❌ Could not create directory '{custom_path}': {e}\"\n",
    "                yield None, None, None, update_architecture_text(), msg\n",
    "                return\n",
    "\n",
    "        # 6) Build model\n",
    "        lr = float(lr)\n",
    "        if torch.cuda.is_available():\n",
    "            try:\n",
    "                torch.cuda.synchronize()  # Will raise if CUDA state is bad\n",
    "            except RuntimeError as e:\n",
    "                raise gr.Error(\"❌ CUDA failure detected. Please restart the dashboard or kernel.\")\n",
    "\n",
    "        model = build_model().to(device)\n",
    "\n",
    "        # 7) Validate forward pass\n",
    "        if file.endswith(\".csv\"):\n",
    "            df = pd.read_csv(file)\n",
    "            if 'y' not in df.columns:\n",
    "                yield None, None, None, update_architecture_text(), \"❌ CSV missing 'y' column.\"\n",
    "                return\n",
    "            num_features = df.shape[1] - 1\n",
    "            is_valid, error_msg, bad_layer_idx = validate_model_forward_pass(model, \"tabular\", num_features=num_features)\n",
    "        else:\n",
    "            is_valid, error_msg, bad_layer_idx = validate_model_forward_pass(model, \"image\", image_size=int(image_size), num_channels=num_channels)\n",
    "\n",
    "        if not is_valid:\n",
    "            # highlight offending layer + yield\n",
    "            updated_view = update_architecture_text(highlight_index=bad_layer_idx)\n",
    "            yield None, None, None, updated_view, \"\"\n",
    "            return\n",
    "\n",
    "        # 8) Check for trainable params\n",
    "        if not any(p.requires_grad for p in model.parameters()):\n",
    "            yield None, None, None, update_architecture_text(), \"⚠️ Model has no trainable parameters. Add a Linear or Conv2d layer.\"\n",
    "            return\n",
    "\n",
    "        # 9) Validate batch_size\n",
    "        try:\n",
    "            batch_size = int(batch_size)\n",
    "            if batch_size <= 0:\n",
    "                yield None, None, None, update_architecture_text(), \"❌ Batch size must be a positive integer\"\n",
    "                return\n",
    "        except:\n",
    "            yield None, None, None, update_architecture_text(), \"❌ Batch size must be a valid number\"\n",
    "            return\n",
    "\n",
    "        # 10) Validate image_size\n",
    "        try:\n",
    "            image_size = int(image_size)\n",
    "            if image_size <= 0:\n",
    "                yield None, None, None, update_architecture_text(), \"❌ Image size must be a positive integer\"\n",
    "                return\n",
    "        except:\n",
    "            yield None, None, None, update_architecture_text(), \"❌ Image size must be a valid number\"\n",
    "            return\n",
    "\n",
    "        # 11) Load data\n",
    "        loss_fn = nn.MSELoss() if loss_name == 'MSELoss' else nn.CrossEntropyLoss()\n",
    "        data = load_data(file, custom_path, batch_size=batch_size, image_size=image_size, num_channels=channels, loss_fn = loss_fn)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr) if opt_name == 'SGD' else optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        loss_history = []\n",
    "        weight_path = []\n",
    "        status_lines = []\n",
    "        \n",
    "        # 12) Train\n",
    "\n",
    "        if data[\"type\"] == \"tabular\":\n",
    "            X_train, y_train = data[\"train\"]\n",
    "        \n",
    "            # Convert full dataset tensors (before DataLoader)\n",
    "            X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "        \n",
    "            if isinstance(loss_fn, nn.MSELoss):\n",
    "                y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)  \n",
    "            elif isinstance(loss_fn, nn.CrossEntropyLoss):\n",
    "                y_train = torch.tensor(y_train, dtype=torch.long).view(-1)  \n",
    "            elif isinstance(loss_fn, nn.BCEWithLogitsLoss):\n",
    "                y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)  \n",
    "            else:\n",
    "                yield None, None, None, update_architecture_text(), \"❌ Error loading training data\"\n",
    "                return\n",
    "        \n",
    "            # DataLoader\n",
    "            train_dataset = TensorDataset(X_train, y_train)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "            for epoch in range(1, max_epochs + 1):\n",
    "                epoch_loss = 0\n",
    "                num_batches = 0\n",
    "        \n",
    "                for X_batch, y_batch in train_loader:\n",
    "                    X_batch = X_batch.to(device)\n",
    "                    y_batch = y_batch.to(device)\n",
    "        \n",
    "                    optimizer.zero_grad()\n",
    "        \n",
    "                    if isinstance(loss_fn, nn.MSELoss):\n",
    "                        y_input = y_batch.float().view(-1, 1)\n",
    "                    elif isinstance(loss_fn, nn.CrossEntropyLoss):\n",
    "                        y_input = y_batch.long().view(-1)  # CrossEntropy expects class indices, not 2D\n",
    "                    elif isinstance(loss_fn, nn.BCEWithLogitsLoss):\n",
    "                        y_input = y_batch.float().view(-1, 1)\n",
    "                    else:\n",
    "                        yield None, None, None, update_architecture_text(), \"❌ Unhandled Loss Function\"\n",
    "                        return\n",
    "        \n",
    "                    \n",
    "                    out = model(X_batch)\n",
    "                    loss = loss_fn(out, y_input)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "        \n",
    "                    epoch_loss += loss.item()\n",
    "                    num_batches += 1\n",
    "        \n",
    "                    weight_path.append(get_flat_weights(model).cpu().numpy())\n",
    "        \n",
    "                avg_epoch_loss = epoch_loss / num_batches\n",
    "                loss_history.append(avg_epoch_loss)\n",
    "        \n",
    "                status_lines.append(f\"Epoch {epoch}/{max_epochs} — Loss: {avg_epoch_loss:.4f}\")\n",
    "                yield None, None, None, update_architecture_text(), \"\\n\\n\".join(status_lines)\n",
    "\n",
    "        else:\n",
    "            # image data\n",
    "            train_loader = data[\"train\"]\n",
    "            for epoch in range(1, max_epochs + 1):\n",
    "                epoch_loss = 0\n",
    "                num_batches = 0\n",
    "                for X_batch, y_batch in train_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    X_batch = X_batch.to(device)\n",
    "                    y_batch = y_batch.to(device)\n",
    "\n",
    "                    if isinstance(loss_fn, nn.MSELoss):\n",
    "                        # One-hot for MSE\n",
    "                        y_input = torch.nn.functional.one_hot(\n",
    "                            y_batch,\n",
    "                            num_classes=len(torch.unique(y_batch))\n",
    "                        ).float().to(device)\n",
    "                    else:\n",
    "                        y_input = y_batch\n",
    "\n",
    "                    out = model(X_batch)\n",
    "                    if isinstance(loss_fn, nn.MSELoss):\n",
    "                        out = torch.softmax(out, dim=1)\n",
    "\n",
    "                    loss = loss_fn(out, y_input)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    epoch_loss += loss.item()\n",
    "                    num_batches += 1\n",
    "\n",
    "                    weight_path.append(get_flat_weights(model).cpu().numpy())\n",
    "\n",
    "\n",
    "                # Average loss for the epoch\n",
    "                avg_epoch_loss = epoch_loss / num_batches\n",
    "                loss_history.append(avg_epoch_loss)\n",
    "\n",
    "                status_lines.append(f\"Epoch {epoch}/{max_epochs} — Loss: {avg_epoch_loss:.4f}\")\n",
    "                yield None, None, None, update_architecture_text(), \"\\n\\n\".join(status_lines)\n",
    "\n",
    "            # Cleanup extracted images\n",
    "            if data[\"path\"]:\n",
    "                try:\n",
    "                    shutil.rmtree(data[\"path\"])\n",
    "                except Exception as e:\n",
    "                    gr.Warning(f\"Warning: Could not delete extracted folder: {e}\")\n",
    "\n",
    "        loss_plot_path = os.path.join(OUTPUT_DIR, \"loss_plot.png\")\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(loss_history)\n",
    "        ax.set_title(\"Loss over Epochs\")\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        plt.savefig(loss_plot_path)\n",
    "        plt.close(fig)\n",
    "        animation_path = os.path.join(OUTPUT_DIR, \"animation.mp4\")\n",
    "\n",
    "\n",
    "        if generate_animation:\n",
    "\n",
    "            generate_3d_animation_pca(\n",
    "            np.array(weight_path),\n",
    "            loss_history,\n",
    "            animation_path, \n",
    "            target_frames=target_frames,\n",
    "            frame_rate=frame_rate\n",
    "            )\n",
    "        else:\n",
    "            create_dummy_video(animation_path)\n",
    "        # Save the trained model in OUTPUT_DIR\n",
    "        model_path = os.path.join(OUTPUT_DIR, \"trained_model.pt\")\n",
    "        torch.save(model, model_path)\n",
    "\n",
    "        final_logs = \"\\n\".join(status_lines)\n",
    "\n",
    "        for path in [loss_plot_path, animation_path, model_path]:\n",
    "            if os.path.isdir(path):\n",
    "                gr.Warning(f\"❌ Output path is a directory, expected a file: {path}\")\n",
    "        \n",
    "        del model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        yield loss_plot_path,  animation_path, model_path, update_architecture_text(), final_logs\n",
    "\n",
    "    except gr.Error as e:\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        raise gr.Error(f\"❌ Unexpected error: {str(e)}\")\n",
    "\n",
    "# This function gets a subfolder in the home directory which is not protected and is writeable\n",
    "# Input: Nothing\n",
    "# Output: A writeable folder\n",
    "def get_default_writable_folder():\n",
    "\n",
    "    home_dir = os.path.expanduser(\"~\")  # e.g. C:\\\\Users\\\\Alice on Windows\n",
    "    default_path = os.path.join(home_dir, \"my_gradio_data\")\n",
    "    os.makedirs(default_path, exist_ok=True)\n",
    "    return default_path\n",
    "\n",
    "# This function wraps the training operation, ensures a valid save path, and yields training outputs\n",
    "# Input: loss_name, opt_name, lr, batch_size, image_size, \n",
    "                                  #file, custom_path, epochs, num_channels, \n",
    "                                 # generate_animation, target_frames, frame_rate\n",
    "# Output: A generator which returns training progress outputs from train_model\n",
    "def train_model_with_default_path(loss_name, opt_name, lr, batch_size, image_size, \n",
    "                                  file, custom_path, epochs, num_channels, \n",
    "                                  generate_animation, target_frames, frame_rate\n",
    "):\n",
    "    if not custom_path or custom_path.strip() == \"\":\n",
    "        custom_path = get_default_writable_folder()\n",
    "\n",
    "    for item in train_model(loss_name, opt_name, lr, \n",
    "                            batch_size, image_size, file, \n",
    "                            custom_path, epochs, num_channels, \n",
    "                            generate_animation, target_frames, frame_rate\n",
    "    ):\n",
    "        yield item\n",
    "\n",
    "\n",
    "# This function creates a dummy video to prevent the gradio frontend from crashing if the user selects no video\n",
    "# Input: Output_path to put the video in\n",
    "# Output: A black video for the gradio screen\n",
    "def create_dummy_video(output_path):\n",
    "    command = [\n",
    "        \"ffmpeg\",\n",
    "        \"-f\", \"lavfi\",\n",
    "        \"-i\", \"color=c=black:s=1280x720:d=5\", \n",
    "        \"-c:v\", \"libx264\",\n",
    "        \"-t\", \"5\",\n",
    "        \"-pix_fmt\", \"yuv420p\",\n",
    "        \"-y\", output_path\n",
    "    ]\n",
    "    try:\n",
    "        result = subprocess.run(command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        gr.Warning(f\"❌ Error creating dummy video: {e.stderr.decode()}\")\n",
    "\n",
    "# --------------------------------\n",
    "# Gradio wiring\n",
    "# --------------------------------\n",
    "\n",
    "with gr.Blocks() as dashboard:\n",
    "\n",
    "    with gr.Tab(\"Build Model\"):\n",
    "        gr.Markdown(\"## Build a Layer\")\n",
    "        builder_arch = gr.Textbox(label=\"Architecture So Far\", lines=6)\n",
    "\n",
    "        layer_type_dropdown = gr.Dropdown(choices=list(layer_map.keys()), label=\"Layer Type\", value=\"Linear\")\n",
    "        in_dim = gr.Textbox(label=\"Input Dim (Linear/Conv2d)\")\n",
    "        out_dim = gr.Textbox(label=\"Output Dim (Linear/Conv2d)\")\n",
    "\n",
    "        conv_kernel = gr.Textbox(label=\"Kernel Size\", value=\"3\", visible=False)\n",
    "        conv_padding = gr.Textbox(label=\"Padding\", value=\"1\", visible=False)\n",
    "        conv_stride = gr.Textbox(label=\"Stride\", value=\"1\", visible=False)\n",
    "        conv_bias = gr.Checkbox(label=\"Include Bias\", value=True, visible=False)\n",
    "\n",
    "        pool_kernel = gr.Textbox(label=\"Pool Kernel Size\", value=\"2\", visible=False)\n",
    "        pool_stride = gr.Textbox(label=\"Stride\", value=\"2\", visible=False)\n",
    "        pool_padding = gr.Textbox(label=\"Padding\", value=\"0\", visible=False)\n",
    "\n",
    "        avgpool_kernel = gr.Textbox(label=\"AvgPool Kernel Size\", value=\"2\", visible=False)\n",
    "        avgpool_stride = gr.Textbox(label=\"Stride\", value=\"2\", visible=False)\n",
    "        avgpool_padding = gr.Textbox(label=\"Padding\", value=\"0\", visible=False)\n",
    "\n",
    "        leaky_relu_slope = gr.Textbox(label=\"Negative Slope\", value=\"0.01\", visible=False)\n",
    "        elu_alpha = gr.Textbox(label=\"ELU Alpha\", value=\"1.0\", visible=False)\n",
    "\n",
    "        add_btn = gr.Button(\"Add Layer\")\n",
    "        add_btn.click(\n",
    "            fn=add_layer,\n",
    "            inputs=[\n",
    "                layer_type_dropdown, in_dim, out_dim,\n",
    "                conv_kernel, conv_padding, conv_stride, conv_bias,\n",
    "                pool_kernel, pool_stride, pool_padding,\n",
    "                avgpool_kernel, avgpool_stride, avgpool_padding,\n",
    "                leaky_relu_slope, elu_alpha\n",
    "            ],\n",
    "            outputs=builder_arch\n",
    "        )\n",
    "\n",
    "        def toggle_fields(layer_type):\n",
    "            is_conv = (layer_type == \"Conv2d\")\n",
    "            is_pool = (layer_type == \"MaxPool2d\")\n",
    "            is_avgpool = (layer_type == \"AvgPool2d\")\n",
    "            is_leaky = (layer_type == \"LeakyReLU\")\n",
    "            is_elu = (layer_type == \"ELU\")\n",
    "            return [\n",
    "                gr.update(visible=is_conv),\n",
    "                gr.update(visible=is_conv),\n",
    "                gr.update(visible=is_conv),\n",
    "                gr.update(visible=is_conv),\n",
    "                gr.update(visible=is_pool),\n",
    "                gr.update(visible=is_pool),\n",
    "                gr.update(visible=is_pool),\n",
    "                gr.update(visible=is_avgpool),\n",
    "                gr.update(visible=is_avgpool),\n",
    "                gr.update(visible=is_avgpool),\n",
    "                gr.update(visible=is_leaky),\n",
    "                gr.update(visible=is_elu),\n",
    "            ]\n",
    "\n",
    "        layer_type_dropdown.change(\n",
    "            toggle_fields,\n",
    "            inputs=[layer_type_dropdown],\n",
    "            outputs=[\n",
    "                conv_kernel, conv_padding, conv_stride, conv_bias,\n",
    "                pool_kernel, pool_stride, pool_padding,\n",
    "                avgpool_kernel, avgpool_stride, avgpool_padding,\n",
    "                leaky_relu_slope, elu_alpha\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        reset_btn = gr.Button(\"Reset Layers\")\n",
    "        reset_btn.click(fn=reset_layers, inputs=[], outputs=builder_arch)\n",
    "\n",
    "        gr.Markdown(\"### Edit or Delete a Layer\")\n",
    "        layer_index = gr.Number(label=\"Layer Index (0-based)\", precision=0)\n",
    "        new_layer_type = gr.Dropdown(list(layer_map.keys()), label=\"New Layer Type\")\n",
    "        new_in_dim = gr.Textbox(label=\"New Input Dim (if applicable)\")\n",
    "        new_out_dim = gr.Textbox(label=\"New Output Dim (if applicable)\")\n",
    "\n",
    "        edit_kernel = gr.Textbox(label=\"Kernel Size\", value=\"3\", visible=False)\n",
    "        edit_padding = gr.Textbox(label=\"Padding\", value=\"1\", visible=False)\n",
    "        edit_stride = gr.Textbox(label=\"Stride\", value=\"1\", visible=False)\n",
    "        edit_bias = gr.Checkbox(label=\"Include Bias\", value=True, visible=False)\n",
    "\n",
    "        edit_pool_kernel = gr.Textbox(label=\"Pool Kernel Size\", value=\"2\", visible=False)\n",
    "        edit_pool_stride = gr.Textbox(label=\"Stride\", value=\"2\", visible=False)\n",
    "        edit_pool_padding = gr.Textbox(label=\"Padding\", value=\"0\", visible=False)\n",
    "\n",
    "        edit_avgpool_kernel = gr.Textbox(label=\"AvgPool Kernel Size\", value=\"2\", visible=False)\n",
    "        edit_avgpool_stride = gr.Textbox(label=\"Stride\", value=\"2\", visible=False)\n",
    "        edit_avgpool_padding = gr.Textbox(label=\"Padding\", value=\"0\", visible=False)\n",
    "\n",
    "        edit_leaky_relu_slope = gr.Textbox(label=\"Negative Slope\", value=\"0.01\", visible=False)\n",
    "        edit_elu_alpha = gr.Textbox(label=\"ELU Alpha\", value=\"1.0\", visible=False)\n",
    "\n",
    "        edit_btn = gr.Button(\"Edit Layer\")\n",
    "        delete_btn = gr.Button(\"Delete Layer\")\n",
    "        insert_btn = gr.Button(\"Insert New Layer\")\n",
    "\n",
    "        edit_btn.click(\n",
    "            fn=update_layer,\n",
    "            inputs=[\n",
    "                layer_index, new_layer_type, new_in_dim, new_out_dim,\n",
    "                edit_kernel, edit_padding, edit_stride, edit_bias,\n",
    "                edit_pool_kernel, edit_pool_stride, edit_pool_padding,\n",
    "                edit_avgpool_kernel, edit_avgpool_stride, edit_avgpool_padding,\n",
    "                edit_leaky_relu_slope, edit_elu_alpha\n",
    "            ],\n",
    "            outputs=builder_arch\n",
    "        )\n",
    "\n",
    "        delete_btn.click(fn=delete_layer, inputs=[layer_index], outputs=builder_arch)\n",
    "        insert_btn.click(\n",
    "            fn=insert_layer,\n",
    "            inputs=[\n",
    "                layer_index, new_layer_type, new_in_dim, new_out_dim,\n",
    "                edit_kernel, edit_padding, edit_stride, edit_bias,\n",
    "                edit_pool_kernel, edit_pool_stride, edit_pool_padding,\n",
    "                edit_avgpool_kernel, edit_avgpool_stride, edit_avgpool_padding,\n",
    "                edit_leaky_relu_slope, edit_elu_alpha\n",
    "            ],\n",
    "            outputs=builder_arch\n",
    "        )\n",
    "\n",
    "        def toggle_edit_fields(layer_type):\n",
    "            is_conv = (layer_type == \"Conv2d\")\n",
    "            is_pool = (layer_type == \"MaxPool2d\")\n",
    "            is_avgpool = (layer_type == \"AvgPool2d\")\n",
    "            is_leaky = (layer_type == \"LeakyReLU\")\n",
    "            is_elu = (layer_type == \"ELU\")\n",
    "            return [\n",
    "                gr.update(visible=is_conv),\n",
    "                gr.update(visible=is_conv),\n",
    "                gr.update(visible=is_conv),\n",
    "                gr.update(visible=is_conv),\n",
    "                gr.update(visible=is_pool),\n",
    "                gr.update(visible=is_pool),\n",
    "                gr.update(visible=is_pool),\n",
    "                gr.update(visible=is_avgpool),\n",
    "                gr.update(visible=is_avgpool),\n",
    "                gr.update(visible=is_avgpool),\n",
    "                gr.update(visible=is_leaky),\n",
    "                gr.update(visible=is_elu),\n",
    "            ]\n",
    "\n",
    "        new_layer_type.change(\n",
    "            toggle_edit_fields,\n",
    "            inputs=[new_layer_type],\n",
    "            outputs=[\n",
    "                edit_kernel, edit_padding, edit_stride, edit_bias,\n",
    "                edit_pool_kernel, edit_pool_stride, edit_pool_padding,\n",
    "                edit_avgpool_kernel, edit_avgpool_stride, edit_avgpool_padding,\n",
    "                edit_leaky_relu_slope, edit_elu_alpha\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Device info row\n",
    "        with gr.Row():\n",
    "            device_output = gr.Markdown(get_device_status())\n",
    "            refresh_button = gr.Button(\"🔄 Refresh Device Status\")\n",
    "            refresh_button.click(fn=get_device_status, inputs=[], outputs=device_output)\n",
    "\n",
    "    with gr.Tab(\"Train\"):\n",
    "        gr.Markdown(\"## Train the Model\")\n",
    "\n",
    "       \n",
    "        loss_dropdown = gr.Dropdown(['MSELoss', 'CrossEntropyLoss'], label='Loss Function')\n",
    "        opt_dropdown = gr.Dropdown(['SGD', 'Adam'], label='Optimizer')\n",
    "        lr_box = gr.Textbox(value=\"0.01\", label=\"Learning Rate\")\n",
    "        batch_box = gr.Textbox(value=\"32\", label=\"Batch Size\")\n",
    "        size_box = gr.Textbox(value=\"28\", label=\"Image Resize (e.g. 28x28)\")\n",
    "        file_box = gr.Textbox(label=\"Path to CSV or ZIP\")\n",
    "        custom_box = gr.Textbox(label=\"Custom Extraction Path (optional)\")\n",
    "        epochs_box = gr.Textbox(value=\"100\", label=\"Epochs\")\n",
    "        generate_3d_checkbox = gr.Checkbox(label=\"Generate 3D Descent Animation (⚠️ Slower, CPU/RAM-intensive)\", value=False)\n",
    "        generate_3d_targetframes = gr.Textbox(value = \"300\", label = \"Target Frames for Video\")\n",
    "        generate_3d_framerate = gr.Textbox(value=\"10\", label=\"Frame Rate (Frames per Second)\")\n",
    "        channel_dropdown = gr.Dropdown([1, 3], label=\"Input Channels (1 = Grayscale, 3 = RGB)\", value=3)\n",
    "\n",
    "        # Outputs\n",
    "        loss_curve = gr.Image(label=\"Loss Curve\")\n",
    "        animation_video = gr.Video(label=\"3D Descent Animation\")\n",
    "        model_file = gr.File(label=\"Download Trained Model\")  \n",
    "        log_box = gr.Markdown(label=\"Log\")\n",
    "        train_button = gr.Button(\"Start Training\")\n",
    "        train_button.click(\n",
    "            fn=train_model_with_default_path,\n",
    "            inputs=[\n",
    "                loss_dropdown,\n",
    "                opt_dropdown,\n",
    "                lr_box,\n",
    "                batch_box,\n",
    "                size_box,\n",
    "                file_box,\n",
    "                custom_box,\n",
    "                epochs_box,\n",
    "                channel_dropdown,\n",
    "                generate_3d_checkbox,\n",
    "                generate_3d_targetframes,\n",
    "                generate_3d_framerate\n",
    "\n",
    "            ],\n",
    "            outputs=[\n",
    "                loss_curve,\n",
    "                animation_video,\n",
    "                model_file,\n",
    "                builder_arch,\n",
    "                log_box\n",
    "            ]\n",
    "        )\n",
    "\n",
    "dashboard.queue()\n",
    "dashboard.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5dd49f-c6cc-4837-a0e3-d50e6cc048af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
