{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12a4d6bf-3611-4b3f-8917-c4cdbf04fc1d",
   "metadata": {},
   "source": [
    "# Solving Wordle with Actor Critic Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d25a30-0f3e-4678-be57-27926fd87f8d",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Wordle is a word-guessing game where the player has six attempts to identify a hidden five-letter target word. After each guess, feedback is given for every letter: gray if the letter does not occur in the target, yellow if it occurs in a different position, and green if it is in the correct position. This feedback defines the game’s evolving state.\n",
    "\n",
    "We formulate Wordle as a reinforcement learning (RL) problem. Each state corresponds to the history of guesses and feedback, while actions are legal word choices from the vocabulary. The agent’s objective is to select actions that reduce the set of candidate target words, ultimately converging on the correct solution. The environment, optimization function, and hyperparameters are fixed; the agent must learn a strategy through exploration.\n",
    "\n",
    "Although Wordle can be solved deterministically with hard-coded algorithms such as Knuth’s Mastermind strategy, those methods are brittle and scale poorly beyond small, well-structured state spaces. By contrast, an RL approach offers greater generalization. While it may not outperform Knuth at Wordle’s current scale, it provides a framework adaptable to larger vocabularies or related problems where hard-coded search becomes infeasible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f484cb-9f51-4113-a883-56674df5f845",
   "metadata": {},
   "source": [
    "---\n",
    "# Using Reinforcement Learning on Wordle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83780f17-efbd-4726-be50-2102b718a265",
   "metadata": {},
   "source": [
    "## Why Actor–Critic Methods Were Used\n",
    "\n",
    "This notebook solves Wordle using an actor–critic architecture combined with Generalized Advantage Estimation (GAE) and Proximal Policy Optimization (PPO). Actor–critic methods train two networks:\n",
    "\n",
    "- Actor: parameterizes the policy $\\pi_\\theta(a \\mid s)$, mapping states to action probabilities.\n",
    "\n",
    "- Critic: estimates the value function $V_\\phi(s)$, providing a learned baseline to reduce variance in policy gradient updates.\n",
    "\n",
    "Compared to REINFORCE, which relies solely on Monte Carlo returns, actor–critic methods substantially reduce gradient variance by incorporating value-function baselines. This stabilizes learning, especially in environments like Wordle where rewards are sparse and heavily delayed.\n",
    "\n",
    "To further improve stability, we adopt:\n",
    "\n",
    "GAE: computes smoothed, lower-variance advantage estimates $A_t^{(\\lambda)}$, balancing bias and variance.\n",
    "\n",
    "PPO: introduces a clipped surrogate objective and adaptive KL control, preventing destructive policy updates while maintaining sample efficiency.\n",
    "\n",
    "Together, these choices address the high variance of pure REINFORCE and yield a more stable and scalable training procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9b6ac7-0ad8-4308-8163-35eb95dc6fec",
   "metadata": {},
   "source": [
    "---\n",
    "# Mathematical Notes for the Wordle RL Agent (PPO + GAE, Masked Softmax)\n",
    "\n",
    "This note documents the exact mathematics used in the Wordle reinforcement learning (RL) agent that trains with PPO (clipped policy gradients) and GAE, with hard action masking to enforce Wordle feedback consistency. \n",
    "\n",
    "## Environment\n",
    "### Wordle as an MDP\n",
    "We model Wordle as a finite-horizon episodic MDP with horizon $T=6$ turns.\n",
    "\n",
    "$\\mathcal{A}$ is the legal 5-letter vocabulary (the **allowed** list). Each action $a \\in \\mathcal{A}$ is a guess (a word).\n",
    "``Target set`` $\\mathcal{S}$ (``solutions``) is the subset of canonical Wordle answers. At the start of an episode a target word $w^\\star \\in \\mathcal{S}$ is sampled.\n",
    "``State $s_t$`` encodes the guess/feedback history up to turn $t$: $$s_t = \\big((a_1, y_1), (a_2,y_2), \\dots, (a_{t-1}, y_{t-1})\\big).$$\n",
    " ``Transition`` is deterministic given $(s_t, a_t)$: the environment returns the Wordle feedback $$y_t \\in \\{0,1,2\\}^5,$$ where $0$=gray, $1$=yellow, $2$=green, computed by the standard Wordle scoring function $f(a_t, w^\\star)$.\n",
    "  ``Termination`` occurs at the first $t$ such that $a_t = w^\\star$ (win) or when $t = T$ (loss).\n",
    "\n",
    "## Posterior Candidate Set and Hard Masking\n",
    "Let $\\mathcal{C}_1 = \\mathcal{S}$ denote the initial candidate set of possible targets. After observing $(a_t, y_t)$ we deterministically update\n",
    "$$\n",
    "\\mathcal{C}_{t+1} \\;=\\; \\{\\, w \\in \\mathcal{C}_t \\;:\\; f(a_t, w) = y_t \\,\\}.\n",
    "$$\n",
    "Thus the **sufficient statistic** for optimal play is $\\mathcal{C}_t$. The code uses **hard action masking**. We define a mask\n",
    "$$\n",
    "m_t(a) \\;=\\;\n",
    "\\begin{cases}\n",
    "1, & \\text{if } a \\text{ is consistent with } \\{(a_\\tau,y_\\tau)\\}_{\\tau \\le t} \\text{ (i.e., } a \\in \\text{Posterior-Consistent}\\text{)},\\\\[4pt]\n",
    "0, & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "The policy's probability mass over invalid actions is forced to zero by masked softmax (see Policy, Value, and Masked Softmax). For diagnostics, the code logs the **pre-mask invalid mass**, i.e. the policy mass that would have been assigned to invalid actions before applying the mask.\n",
    "\n",
    "## Policy, Value, and Masked Softmax\n",
    "### Masked Softmax\n",
    "Let the actor network produce unnormalized logits $z_\\theta(s_t)\\in\\mathbb{R}^{|\\mathcal{A}|}$. We define masked logits as:\n",
    "$$\n",
    "\\tilde z_\\theta(a \\mid s_t) \\;=\\;\n",
    "\\begin{cases}\n",
    "z_\\theta(a \\mid s_t), & m_t(a)=1,\\\\\n",
    "-\\infty, & m_t(a)=0.\n",
    "\\end{cases}\n",
    "$$\n",
    "Then the policy is\n",
    "$$\n",
    "\\pi_\\theta(a\\mid s_t) \\;=\\; \\frac{\\exp\\big(\\tilde z_\\theta(a \\mid s_t)/\\tau\\big)}{\\sum_{a'\\in\\mathcal{A}}\\exp\\big(\\tilde z_\\theta(a' \\mid s_t)/\\tau\\big)},\n",
    "$$\n",
    "with temperature $\\tau>0$ (often $\\tau=1$ during training and lower at evaluation for greedier behavior). By construction, $\\pi_\\theta(a\\mid s_t)=0$ when $m_t(a)=0$.\n",
    "\n",
    "### Value function\n",
    "A critic $V_\\phi(s_t)$ estimates the state value under the current policy:\n",
    "$$\n",
    "V_\\phi(s_t) \\;\\approx\\; \\mathbb{E}_{\\pi_\\theta}\\big[\\,R_t \\,\\big|\\, s_t\\big],\n",
    "$$\n",
    "where $R_t$ is the return defined below.\n",
    "\n",
    "## Rewards and Returns\n",
    "### Shaped reward\n",
    "The code employs dense shaping to stabilize learning in a sparse terminal-reward problem. Let $\\mathcal{C}_t$ be the candidate set size after applying all $(a_\\tau,y_\\tau)$ up to $t{-}1$. An information-gain shaped reward is\n",
    "$$\n",
    "r_t^{(\\text{IG})} \\;=\\; \\log |\\mathcal{C}_t| \\;-\\; \\log |\\mathcal{C}_{t+1}|.\n",
    "$$\n",
    "This measures the reduction in posterior entropy under a uniform prior on $\\mathcal{C}_t$. Additional components (e.g., small per-green/yellow bonuses, or an on-win bonus at termination) can be linearly combined:\n",
    "$$\n",
    "r_t \\;=\\; \\alpha \\, r_t^{(\\text{IG})} \\;+\\; \\beta_{\\text{term}}\\cdot \\mathbf{1}\\{a_t=w^\\star\\}\\cdot \\mathbf{1}\\{t\\le T\\} \\;+\\; \\sum_k \\gamma_k \\, r_t^{(k)}.\n",
    "$$\n",
    "In practice $\\alpha>0$, $\\beta_{\\text{term}}\\ge 0$; other terms $r_t^{(k)}$ are optional.\n",
    "\n",
    "### Undiscounted Episodic Return\n",
    "Episodes are short; the implementation uses $\\gamma=1$ (undiscounted):\n",
    "$$\n",
    "R_t \\;=\\; \\sum_{u=t}^{t_{\\text{end}}} r_u,\n",
    "$$\n",
    "where $t_{\\text{end}}\\le T$ is the termination time.\n",
    "\n",
    "## Advantage Estimation (GAE)\n",
    "Define the TD residuals\n",
    "$$\n",
    "\\delta_t \\;=\\; r_t \\;+\\; \\gamma \\, V_\\phi(s_{t+1}) \\;-\\; V_\\phi(s_t),\n",
    "$$\n",
    "with $\\gamma=1$ here. Generalized Advantage Estimation (GAE) with parameter $\\lambda \\in [0,1]$ is\n",
    "$$\n",
    "A_t^{(\\lambda)} \\;=\\; \\sum_{l=0}^{\\infty} (\\gamma\\lambda)^l \\, \\delta_{t+l}.\n",
    "$$\n",
    "In finite horizon, the sum truncates at episode end. Advantages are normalized per-batch in code:\n",
    "$$\n",
    "\\hat A_t \\;=\\; \\frac{A_t^{(\\lambda)} - \\mu_A}{\\sigma_A + \\varepsilon}.\n",
    "$$\n",
    "\n",
    "## PPO Objective with Clipping\n",
    "Let the importance ratio be\n",
    "$$\n",
    "r_t(\\theta) \\;=\\; \\frac{\\pi_\\theta(a_t\\mid s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t\\mid s_t)}.\n",
    "$$\n",
    "The clipped surrogate objective uses a symmetric clip parameter $\\varepsilon>0$:\n",
    "$$\n",
    "L^{\\text{CLIP}}(\\theta) \\;=\\; \\mathbb{E}_t \\Big[ \\min\\Big( r_t(\\theta)\\,\\hat A_t,\\; \\mathrm{clip}\\big(r_t(\\theta),\\, 1{-}\\varepsilon,\\, 1{+}\\varepsilon\\big)\\,\\hat A_t \\Big) \\Big].\n",
    "$$\n",
    "The critic is trained by a (possibly clipped) squared-error objective\n",
    "$$\n",
    "L^{\\text{V}}(\\phi) \\;=\\; \\mathbb{E}_t \\Big[ \\big( V_\\phi(s_t) - \\hat R_t \\big)^2 \\Big],\n",
    "$$\n",
    "where $\\hat R_t$ is the Monte Carlo return or a bootstrapped target consistent with the GAE construction.\n",
    "\n",
    "## Entropy Regularization\n",
    "To discourage premature collapse, add policy entropy $H(\\pi_\\theta(\\cdot\\mid s_t))$ with coefficient $\\beta_{\\text{ent}}$ that is linearly annealed to $0$ during training:\n",
    "$$\n",
    "H(\\pi_\\theta(\\cdot\\mid s)) \\;=\\; - \\sum_{a\\in\\mathcal{A}} \\pi_\\theta(a\\mid s)\\,\\log \\pi_\\theta(a\\mid s).\n",
    "$$\n",
    "\n",
    "## Adaptive KL Control and Actor ``brake``\n",
    "The implementation also monitors minibatch KL divergence\n",
    "$$\n",
    "\\mathrm{KL}\\!\\left(\\pi_{\\theta_{\\text{old}}}(\\cdot\\mid s_t)\\;\\|\\; \\pi_\\theta(\\cdot\\mid s_t)\\right)\n",
    "\\;=\\; \\sum_{a} \\pi_{\\theta_{\\text{old}}}(a\\mid s_t)\\,\n",
    "\\log \\frac{\\pi_{\\theta_{\\text{old}}}(a\\mid s_t)}{\\pi_\\theta(a\\mid s_t)}.\n",
    "$$\n",
    "When the observed KL exceeds a target (e.g., $2.5\\times$ the target), the update \\emph{skips the actor step} (``brake'') and/or increases an adaptive penalty $\\beta_{\\text{KL}}$ or reduces the actor learning rate. This keeps policy updates within a trust region even beyond clipping.\n",
    "\n",
    "### Total Loss (Minimization Form)\n",
    "The code minimizes the following (signs chosen for **minimization**):\n",
    "$$\n",
    "\\mathcal{L}(\\theta,\\phi)\n",
    "\\;=\\;\n",
    "-\\, L^{\\text{CLIP}}(\\theta)\n",
    "\\;+\\;\n",
    "\\beta_{\\text{ent}}\\cdot \\mathbb{E}_t\\!\\left[\\, H\\big(\\pi_\\theta(\\cdot\\mid s_t)\\big) \\right]\n",
    "\\;+\\;\n",
    "c_v \\cdot \\mathbb{E}_t\\!\\left[\\big(V_\\phi(s_t)-\\hat{R}_t\\big)^2\\right]\n",
    "\\;+\\;\n",
    "\\beta_{\\text{KL}} \\cdot \\mathbb{E}_t\\!\\left[\\mathrm{KL}\\!\\left(\\pi_{\\theta_{\\text{old}}} \\,\\|\\, \\pi_\\theta\\right)\\right].\n",
    "$$\n",
    "Here $c_v>0$ is the value-loss weight, $\\beta_{\\text{ent}}\\ge 0$ is annealed to $0$, and $\\beta_{\\text{KL}}\\ge 0$ is adjusted by the controller. In some runs $\\beta_{\\text{KL}}$ may be zero except when KL control activates.\n",
    "\n",
    "## Batching, Epochs, and Optimization\n",
    "- Episodes are collected in batches. Let a batch index set be $\\mathcal{B}$; PPO forms multiple shuffled minibatches per epoch.\n",
    "- The actor parameters $\\theta$ and critic parameters $\\phi$ use \\emph{separate} optimizers and learning rates: $$\\eta_{\\text{actor}} \\neq \\eta_{\\text{critic}}.$$\n",
    "- Gradient clipping is applied: $$\\|\\nabla\\|_2 \\le g_{\\max}.$$\n",
    "- Optional $L_2$ weight decay may be applied to stabilize the critic.\n",
    "- Advantages $\\hat A_t$ are normalized per batch; value targets $\\hat R_t$ use the same rollouts.\n",
    "\n",
    "\n",
    "## Train/Val/Test Protocol\n",
    "The split is applied over **targets** (answers), not the action vocabulary:\n",
    "$$\n",
    "\\text{Train:Val:Test} \\in \\{(0.8,0.1,0.1),\\ (0.7,0.15,0.15)\\}.\n",
    "$$\n",
    "At evaluation time, the policy may be run greedily ($\\arg\\max$) or with low temperature to estimate:\n",
    "- Success rate (SR): fraction of targets solved within $T=6$ turns.\n",
    "- Average turns: mean number of guesses conditioned on success (or overall).\n",
    "- Diagnostics: minibatch KL, entropy $H$, pre-mask invalid mass, critic loss.\n",
    "\n",
    "Confidence intervals (e.g., 95%) for SR are reported with a normal approximation or exact binomial intervals.\n",
    "\n",
    "## Why Masking Matters\n",
    "Without masking, the policy wastes probability mass on invalid guesses (contradicting known feedback), increasing variance and harming sample efficiency. Masked softmax enforces logical consistency with the posterior and empirically accelerates learning. The logit $-\\infty$ implementation is equivalent to zeroing those probabilities and renormalizing over valid actions.\n",
    "\n",
    "## Relation to Optimal Play and Knuth-Style Search\n",
    "An omniscient solver can optimize expected remaining candidate size (or worst-case) via a combinatorial search akin to Knuth's algorithm for Mastermind. The RL agent instead **learns** a policy that implicitly trades off information gain against win probability. The information-gain reward approximates this heuristic: maximizing $$\\mathbb{E}[\\log |\\mathcal{C}_t| - \\log |\\mathcal{C}_{t+1}|]$$ encourages guesses that split the posterior well, while PPO keeps updates stable.\n",
    "\n",
    "## Summary\n",
    "The implementation is a standard PPO+GAE agent specialized to Wordle via hard action masking and information-gain shaping. The total loss minimized in code is exactly\n",
    "$$\n",
    "\\mathcal{L}(\\theta,\\phi)\n",
    "\\;=\\;\n",
    "- L^{\\text{CLIP}}(\\theta)\n",
    "\\;+\\;\n",
    "\\beta_{\\text{ent}}\\cdot \\mathbb{E}_t\\!\\left[ H(\\pi_\\theta(\\cdot\\mid s_t)) \\right]\n",
    "\\;+\\;\n",
    "c_v \\cdot \\mathbb{E}_t\\!\\left[\\big(V_\\phi(s_t)-\\hat{R}_t\\big)^2\\right]\n",
    "\\;+\\;\n",
    "\\beta_{\\text{KL}} \\cdot \\mathbb{E}_t\\!\\left[\\mathrm{KL}\\!\\left(\\pi_{\\theta_{\\text{old}}} \\,\\|\\, \\pi_\\theta\\right)\\right].\n",
    "$$\n",
    "This matches the structure and logging present in the notebook (entropy anneal, KL target/skip, separate actor/critic LRs, masked softmax, and posterior-consistency filter).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25e7a16-a98e-4dcf-b82f-4bcc5672ebb6",
   "metadata": {},
   "source": [
    "---\n",
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d0a3808-d0f3-404b-bd55-b0534db0f7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Split sizes -> train: 1852, val: 231, test: 232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Machine-Learning\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 1852 targets; validating on 231; holding out 232 for FINAL test.\n",
      "[Eval] SR=0.861±0.020  avg_turns=4.63  lrA=3.60e-04 lrC=1.00e-04\n",
      "[Brake] minibatch KL=0.02040 > 2.50×target; skip actor; lrA=4.00e-04 β_KL=0.100 clip=0.090\n",
      "[KLctl] KL=0.00302/0.00800  lrA=7.20e-04  beta=0.063\n",
      "[PPO] batches=5 ep_done=3200/30000 avg_ret_last10=21.52  KL=0.003021  H=4.523  β_KL=0.063 clip=0.120  pre_mask_invalid_mass_mean=0.7694  Vloss=335.481  Ploss=-0.038\n",
      "[Brake] minibatch KL=0.08261 > 2.50×target; skip actor; lrA=3.60e-04 β_KL=0.126 clip=0.090\n",
      "[KLctl] KL=0.00677/0.00800  lrA=8.00e-04  beta=0.041\n",
      "[PPO] batches=10 ep_done=6400/30000 avg_ret_last10=21.81  KL=0.006767  H=4.531  β_KL=0.041 clip=0.120  pre_mask_invalid_mass_mean=0.7678  Vloss=309.940  Ploss=-0.047\n",
      "[Brake] minibatch KL=0.02344 > 2.50×target; skip actor; lrA=4.00e-04 β_KL=0.100 clip=0.090\n",
      "[Brake] minibatch KL=0.03109 > 2.50×target; skip actor; lrA=2.00e-04 β_KL=0.200 clip=0.080\n",
      "[KLctl] KL=0.00556/0.00800  lrA=7.00e-04  beta=0.065\n",
      "[PPO] batches=15 ep_done=9600/30000 avg_ret_last10=22.24  KL=0.005565  H=4.538  β_KL=0.065 clip=0.120  pre_mask_invalid_mass_mean=0.7679  Vloss=278.115  Ploss=-0.046\n",
      "[KLctl] KL=0.00631/0.00800  lrA=8.00e-04  beta=0.016\n",
      "[PPO] batches=20 ep_done=12800/30000 avg_ret_last10=22.60  KL=0.006313  H=4.470  β_KL=0.016 clip=0.120  pre_mask_invalid_mass_mean=0.7687  Vloss=258.011  Ploss=-0.047\n",
      "[Eval] SR=0.866±0.020  avg_turns=4.66  lrA=8.00e-04 lrC=1.00e-04\n",
      "[KLctl] KL=0.00844/0.00800  lrA=8.00e-04  beta=0.011\n",
      "[PPO] batches=25 ep_done=16000/30000 avg_ret_last10=22.78  KL=0.008441  H=4.419  β_KL=0.011 clip=0.120  pre_mask_invalid_mass_mean=0.7690  Vloss=214.522  Ploss=-0.051\n",
      "[Brake] minibatch KL=0.02478 > 2.50×target; skip actor; lrA=4.00e-04 β_KL=0.100 clip=0.090\n",
      "[Brake] minibatch KL=0.03147 > 2.50×target; skip actor; lrA=2.00e-04 β_KL=0.200 clip=0.080\n",
      "[KLctl] KL=0.00638/0.00800  lrA=7.78e-04  beta=0.062\n",
      "[PPO] batches=30 ep_done=19200/30000 avg_ret_last10=22.93  KL=0.006377  H=4.385  β_KL=0.062 clip=0.120  pre_mask_invalid_mass_mean=0.7676  Vloss=214.659  Ploss=-0.049\n",
      "[Brake] minibatch KL=0.02314 > 2.50×target; skip actor; lrA=3.89e-04 β_KL=0.123 clip=0.090\n",
      "[Brake] minibatch KL=0.02006 > 2.50×target; skip actor; lrA=3.50e-04 β_KL=0.156 clip=0.090\n",
      "[KLctl] KL=0.00448/0.00800  lrA=5.04e-04  beta=0.069\n",
      "[PPO] batches=35 ep_done=22400/30000 avg_ret_last10=23.19  KL=0.004482  H=4.220  β_KL=0.069 clip=0.120  pre_mask_invalid_mass_mean=0.7713  Vloss=183.235  Ploss=-0.045\n",
      "[Brake] minibatch KL=0.02321 > 2.50×target; skip actor; lrA=3.02e-04 β_KL=0.100 clip=0.090\n",
      "[KLctl] KL=0.00832/0.00800  lrA=5.44e-04  beta=0.063\n",
      "[PPO] batches=40 ep_done=25600/30000 avg_ret_last10=23.59  KL=0.008319  H=4.103  β_KL=0.063 clip=0.120  pre_mask_invalid_mass_mean=0.7658  Vloss=171.635  Ploss=-0.045\n",
      "[Eval] SR=0.913±0.016  avg_turns=4.44  lrA=5.44e-04 lrC=1.00e-04\n",
      "[Brake] minibatch KL=0.02147 > 2.50×target; skip actor; lrA=2.72e-04 β_KL=0.126 clip=0.090\n",
      "[Brake] minibatch KL=0.02123 > 2.50×target; skip actor; lrA=2.45e-04 β_KL=0.159 clip=0.090\n",
      "[Brake] minibatch KL=0.02007 > 2.50×target; skip actor; lrA=1.22e-04 β_KL=0.300 clip=0.080\n",
      "[KLctl] KL=0.00135/0.00800  lrA=2.20e-04  beta=0.189\n",
      "[PPO] batches=45 ep_done=28800/30000 avg_ret_last10=24.10  KL=0.001355  H=3.748  β_KL=0.189 clip=0.120  pre_mask_invalid_mass_mean=0.7629  Vloss=175.297  Ploss=-0.026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Machine-Learning\\AppData\\Local\\Temp\\ipykernel_2592\\1797369805.py:811: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_path, map_location=agent.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best checkpoint: episodes_done=25600, val_SR=0.913\n",
      "[FINAL TEST] success_rate=0.909±0.037, avg_turns=4.47 on 232 held-out targets (evaluated once)\n",
      "Visualizing a few held-out *test* targets...\n",
      "=== Visualize: target = WORRY ===\n",
      "t1: BLEAH     ⬛⬛⬛⬛⬛   r=  0.50   candidates≈2236\n",
      "t2: TRUCK     ⬛🟨⬛⬛⬛   r=  0.50   candidates≈151\n",
      "t3: ROOPY     🟨🟩⬛⬛🟩   r=  2.53   candidates≈9\n",
      "t4: WORRY     🟩🟩🟩🟩🟩   r= 23.76   candidates≈1\n",
      "Result: ✅ SOLVED in 4 turns.\n",
      "=== Visualize: target = STOKE ===\n",
      "t1: BLEAH     ⬛⬛🟨⬛⬛   r=  0.50   candidates≈2213\n",
      "t2: TRITE     🟨⬛⬛⬛🟩   r=  0.50   candidates≈11\n",
      "t3: STONE     🟩🟩🟩⬛🟩   r=  2.94   candidates≈3\n",
      "t4: STOPE     🟩🟩🟩⬛🟩   r= 16.79   candidates≈2\n",
      "t5: STOVE     🟩🟩🟩⬛🟩   r= 17.00   candidates≈1\n",
      "t6: STOKE     🟩🟩🟩🟩🟩   r= 17.63   candidates≈1\n",
      "Result: ✅ SOLVED in 6 turns.\n",
      "=== Visualize: target = WALTZ ===\n",
      "t1: BLEAH     ⬛🟨⬛🟨⬛   r=  0.50   candidates≈428\n",
      "t2: TRAWL     🟨⬛🟨🟨🟨   r=  1.50   candidates≈2\n",
      "t3: WALTZ     🟩🟩🟩🟩🟩   r= 39.29   candidates≈1\n",
      "Result: ✅ SOLVED in 3 turns.\n",
      "=== Visualize: target = BASTE ===\n",
      "t1: BLEAH     🟩⬛🟨🟨⬛   r=  0.50   candidates≈61\n",
      "t2: BRACE     🟩⬛🟨⬛🟩   r=  2.50   candidates≈5\n",
      "t3: BADGE     🟩🟩⬛⬛🟩   r=  5.37   candidates≈3\n",
      "t4: BASTE     🟩🟩🟩🟩🟩   r= 27.97   candidates≈1\n",
      "Result: ✅ SOLVED in 4 turns.\n",
      "=== Visualize: target = VALET ===\n",
      "t1: BLEAH     ⬛🟨🟨🟨⬛   r=  0.50   candidates≈231\n",
      "t2: DEALT     ⬛🟨🟨🟨🟩   r=  2.00   candidates≈5\n",
      "t3: VALET     🟩🟩🟩🟩🟩   r= 40.45   candidates≈1\n",
      "Result: ✅ SOLVED in 3 turns.\n",
      "=== Visualize: target = JUDGE ===\n",
      "t1: BLEAH     ⬛⬛🟨⬛⬛   r=  0.50   candidates≈2213\n",
      "t2: TRITE     ⬛⬛⬛⬛🟩   r=  0.50   candidates≈157\n",
      "t3: MENSE     ⬛⬛⬛⬛🟩   r=  2.35   candidates≈24\n",
      "t4: COOZE     ⬛⬛⬛⬛🟩   r=  2.29   candidates≈4\n",
      "t5: FUDGE     ⬛🟩🟩🟩🟩   r=  2.00   candidates≈2\n",
      "t6: JUDGE     🟩🟩🟩🟩🟩   r= 18.13   candidates≈1\n",
      "Result: ✅ SOLVED in 6 turns.\n",
      "=== Visualize: target = GAUDY ===\n",
      "t1: BLEAH     ⬛⬛⬛🟨⬛   r=  0.50   candidates≈1851\n",
      "t2: AGUTI     🟨🟨🟩⬛⬛   r=  1.00   candidates≈10\n",
      "t3: GAUDS     🟩🟩🟩🟩⬛   r=  4.16   candidates≈1\n",
      "t4: GAUDY     🟩🟩🟩🟩🟩   r= 34.18   candidates≈1\n",
      "Result: ✅ SOLVED in 4 turns.\n",
      "=== Visualize: target = SHREW ===\n",
      "t1: BLEAH     ⬛⬛🟨⬛🟨   r=  0.50   candidates≈248\n",
      "t2: COHEN     ⬛⬛🟨🟩⬛   r=  1.50   candidates≈45\n",
      "t3: HIKER     🟨⬛⬛🟩🟨   r=  3.75   candidates≈4\n",
      "t4: THREE     ⬛🟩🟩🟩⬛   r=  2.50   candidates≈2\n",
      "t5: SHRED     🟩🟩🟩🟩⬛   r= 10.00   candidates≈1\n",
      "t6: SHREW     🟩🟩🟩🟩🟩   r= 17.63   candidates≈1\n",
      "Result: ✅ SOLVED in 6 turns.\n",
      "=== Visualize: target = GOLLY ===\n",
      "t1: BLEAH     ⬛🟨⬛⬛⬛   r=  0.50   candidates≈595\n",
      "t2: CULLY     ⬛⬛🟩🟩🟩   r=  0.50   candidates≈18\n",
      "t3: WILLY     ⬛⬛🟩🟩🟩   r=  9.50   candidates≈9\n",
      "t4: JOLLY     ⬛🟩🟩🟩🟩   r=  9.08   candidates≈8\n",
      "t5: FOLLY     ⬛🟩🟩🟩🟩   r= 16.10   candidates≈7\n",
      "t6: DOLLY     ⬛🟩🟩🟩🟩   r=-23.89   candidates≈6\n",
      "Result: ❌ FAILED in 6 turns.\n",
      "=== Visualize: target = CLINK ===\n",
      "t1: BLEAH     ⬛🟩⬛⬛⬛   r=  0.50   candidates≈173\n",
      "t2: FLUOR     ⬛🟩⬛⬛⬛   r=  1.50   candidates≈30\n",
      "t3: CLING     🟩🟩🟩🟩⬛   r=  3.45   candidates≈2\n",
      "t4: CLINT     🟩🟩🟩🟩⬛   r= 17.00   candidates≈1\n",
      "t5: CLINK     🟩🟩🟩🟩🟩   r= 22.92   candidates≈1\n",
      "Result: ✅ SOLVED in 5 turns.\n",
      "=== Visualize: target = CRANE ===\n",
      "t1: BLEAH     ⬛⬛🟨🟨⬛   r=  0.50   candidates≈964\n",
      "t2: TARED     ⬛🟨🟨🟨⬛   r=  1.50   candidates≈67\n",
      "t3: SPARE     ⬛⬛🟩🟨🟩   r=  3.13   candidates≈14\n",
      "t4: GRAVE     ⬛🟩🟩⬛🟩   r=  5.61   candidates≈6\n",
      "t5: CRAME     🟩🟩🟩⬛🟩   r= 10.00   candidates≈3\n",
      "t6: CRAZE     🟩🟩🟩⬛🟩   r=-23.21   candidates≈2\n",
      "Result: ❌ FAILED in 6 turns.\n"
     ]
    }
   ],
   "source": [
    "# Wordle RL — PPO-ActorCritic + GAE (unseen-vocab capable)\n",
    "# Safer, stabler training: tighter trust region, bigger batches, entropy floor,\n",
    "# optional KL penalty, reward clipping, annealing, and richer logging.\n",
    "# ============================================\n",
    "\n",
    "import os, math, random, statistics\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Optional\n",
    "import requests\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter as _C\n",
    "\n",
    "# Function: Prefer fast matmul kernels on supported GPUs (e.g., TF32 on Ampere).\n",
    "# Input: none (uses torch backend); guarded by try/except for portability.\n",
    "# Output: none (side-effect is backend precision/perf setting; safe to skip if unsupported).\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Word lists: Allowed (tabatkins) + Solutions (cfreshman)\n",
    "# -----------------------------\n",
    "ALLOWED_URL   = \"https://raw.githubusercontent.com/tabatkins/wordle-list/main/words\"\n",
    "SOLUTIONS_URL = \"https://gist.githubusercontent.com/cfreshman/a03ef2cba789d8cf00c08f767e0fad7b/raw/wordle-answers-alphabetical.txt\"\n",
    "# Allowed list: ~14,855 legal guess words in Wordle (used as the policy's action space).\n",
    "# Solutions list: ~2,315 canonical target words that the game actually selects.\n",
    "\n",
    "# Function: Download a plaintext word list and keep only 5-letter alphabetic words.\n",
    "# Input: url (str) — remote URL with one word per line.\n",
    "# Output: list[str] — lowercase, deduplicated, sorted words.\n",
    "def _download_list(url: str) -> list: \n",
    "    r = requests.get(url, timeout=12)\n",
    "    r.raise_for_status()\n",
    "    words = [w.strip().lower() for w in r.text.splitlines()\n",
    "             if len(w.strip()) == 5 and w.strip().isalpha()]\n",
    "    return sorted(set(words))  # dedupe + sort\n",
    "\n",
    "# Function: Load/caches Wordle vocab and solutions with robust fallbacks.\n",
    "# Input: cache_dir (str) — directory for local \"solutions.txt\" and \"allowed.txt\".\n",
    "# Output: (vocab, solutions)\n",
    "#   - vocab: list[str] — union(allowed, solutions), i.e., the full action space.\n",
    "#   - solutions: list[str] — ground-truth target set for training/eval sampling.\n",
    "def load_wordle_lists(cache_dir: str = \".wordlists\"): \n",
    "    # Attempt to read from cache. If files exist, prefer them (offline-friendly).\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    sol_path     = os.path.join(cache_dir, \"solutions.txt\")\n",
    "    allowed_path = os.path.join(cache_dir, \"allowed.txt\")\n",
    "\n",
    "    solutions, allowed = [], []\n",
    "    # Read cached solutions if present.\n",
    "    if os.path.exists(sol_path):\n",
    "        with open(sol_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            solutions = sorted({w.strip().lower() for w in f if len(w.strip()) == 5 and w.strip().isalpha()})\n",
    "    # Read cached allowed list if present.\n",
    "    if os.path.exists(allowed_path):\n",
    "        with open(allowed_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            allowed = sorted({w.strip().lower() for w in f if len(w.strip()) == 5 and w.strip().isalpha()})\n",
    "    # If missing in cache, download and then cache for future runs.\n",
    "    if not allowed:\n",
    "        try:\n",
    "            allowed = _download_list(ALLOWED_URL)\n",
    "            with open(allowed_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(\"\\n\".join(allowed))\n",
    "        except Exception as e:\n",
    "            allowed = []\n",
    "            print(f\"[warn] Could not fetch allowed list: {e}\")\n",
    "\n",
    "    if not solutions:\n",
    "        try:\n",
    "            solutions = _download_list(SOLUTIONS_URL)\n",
    "            with open(sol_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(\"\\n\".join(solutions))\n",
    "        except Exception as e:\n",
    "            solutions = []\n",
    "            print(f\"[warn] Could not fetch solutions list: {e}\")\n",
    "    # Fallback mode (no net / URLs blocked): try wordfreq, then NLTK.\n",
    "    # Keep only 5-letter alphabetic words; cap solutions to ~3k if synthesized.\n",
    "    if not allowed or not solutions:\n",
    "        print(\"[warn] Falling back to local corpora (wordfreq / nltk). Quality differs from NYT.\")\n",
    "        try:\n",
    "            from wordfreq import top_n_list\n",
    "            wf = [w for w in top_n_list('en', 20000) if len(w) == 5 and w.isalpha()]\n",
    "            if not allowed:   allowed   = sorted(set(wf))\n",
    "            if not solutions: solutions = sorted(set(wf[:3000]))\n",
    "        except Exception as e2:\n",
    "            print(f\"[warn] wordfreq fallback failed: {e2}\")\n",
    "            try:\n",
    "                import nltk; nltk.download('words', quiet=True)\n",
    "                from nltk.corpus import words as nltk_words\n",
    "                wl = sorted(set(w.lower() for w in nltk_words.words() if len(w)==5 and w.isalpha()))\n",
    "                if not allowed:   allowed   = wl\n",
    "                if not solutions: solutions = wl[:min(3000, max(1000, len(wl)//4))]\n",
    "            except Exception as e3:\n",
    "                # If all fallbacks fail, instruct how to install deps.\n",
    "                print(\"[hint] Try: pip install wordfreq nltk\")\n",
    "                raise RuntimeError(\"No word list available. Install `wordfreq` or `nltk`, or enable internet.\") from e3\n",
    "    # Final assembly/validation.\n",
    "    vocab = sorted(set(allowed) | set(solutions))\n",
    "    if not vocab or not solutions:\n",
    "        raise RuntimeError(\"Empty word lists after loading.\")\n",
    "    return vocab, solutions\n",
    "\n",
    "# ---------------------------------\n",
    "# 1) Wordle mechanics: feedback\n",
    "# ---------------------------------\n",
    "\n",
    "# Function: Implements official Wordle scoring (handles duplicates properly).\n",
    "# Input: guess (str), target (str) — both 5-letter words.\n",
    "# Output: Tuple[int,int,int,int,int] — per-position feedback: 0=gray, 1=yellow, 2=green.\n",
    "# Notes: First pass marks greens and collects unmatched target letters; second pass assigns yellows from what's left.\n",
    "def wordle_feedback(guess: str, target: str) -> Tuple[int, int, int, int, int]:\n",
    "    guess, target = guess.lower(), target.lower()\n",
    "    res = [0]*5\n",
    "    unmatched = []\n",
    "    for i,(g,t) in enumerate(zip(guess, target)):\n",
    "        if g == t:\n",
    "            res[i] = 2\n",
    "        else:\n",
    "            unmatched.append(t)\n",
    "    avail = _C(unmatched)\n",
    "    for i,g in enumerate(guess):\n",
    "        if res[i] == 2: continue\n",
    "        if avail[g] > 0:\n",
    "            res[i] = 1; avail[g] -= 1\n",
    "        else:\n",
    "            res[i] = 0\n",
    "    return tuple(res)\n",
    "    \n",
    "# Function: Test whether a candidate word remains logically consistent with all past feedback.\n",
    "# Input: history: list[ (guess:str, feedback:Tuple[int,...]) ], candidate: str (5 letters).\n",
    "# Output: bool — True iff wordle_feedback(guess, candidate) equals recorded feedback for all (guess,feedback) in history.\n",
    "def consistent_with(history: List[Tuple[str, Tuple[int,...]]], candidate: str) -> bool:\n",
    "    return all(wordle_feedback(g, candidate) == fb for (g, fb) in history)\n",
    "    \n",
    "# Function: Filter the vocabulary to words consistent with the entire guess/feedback history.\n",
    "# Input: vocab: list[str] (action space), history: list[(guess, feedback)].\n",
    "# Output: list[str] — posterior candidate set (surviving words).\n",
    "def posterior_candidates(vocab: List[str], history: List[Tuple[str, Tuple[int,...]]]) -> List[str]:\n",
    "    return [w for w in vocab if consistent_with(history, w)]\n",
    "\n",
    "# -------------------------------\n",
    "# 2) State encoder (φ)\n",
    "# -------------------------------\n",
    "\n",
    "class StateEncoder:\n",
    "    # Function: Configure how many past steps to encode and compute state dimension.\n",
    "    # Input: history_len (int) — number of recent (guess,feedback) pairs to include.\n",
    "    # Output: none (sets self.state_dim = 145 * history_len).\n",
    "    def __init__(self, history_len: int = 6):\n",
    "        self.history_len = history_len\n",
    "        self.dim_per_step = 145  # 5*26 (letters) + 5*3 (feedback)\n",
    "        self.state_dim = self.dim_per_step * self.history_len\n",
    "\n",
    "    # Function: Encode up to history_len recent steps into a fixed-length numeric state.\n",
    "    # Input: history: list[(guess:str, feedback:Tuple[int,int,int,int,int])].\n",
    "    # Output: np.ndarray (float32) with shape (state_dim,), where state_dim = history_len * 145.\n",
    "    # Details:\n",
    "    #   - Letters: per position, 26-dim one-hot over a–z (total 5*26=130).\n",
    "    #   - Feedback: per position, 3-dim one-hot over {0,1,2} (total 5*3=15).\n",
    "    #   - Left-pad with all-zero blocks if fewer than history_len steps exist.\n",
    "    def encode(self, history: List[Tuple[str, Tuple[int,...]]]) -> np.ndarray:\n",
    "        blocks = []\n",
    "        cut = history[-self.history_len:]\n",
    "        for guess, fb in cut:\n",
    "            gvec = []\n",
    "            for ch in guess:\n",
    "                oh = [0]*26; idx = ord(ch) - 97\n",
    "                if 0 <= idx < 26: oh[idx] = 1\n",
    "                gvec.extend(oh)\n",
    "            fvec = []\n",
    "            for v in fb:\n",
    "                fo = [0,0,0]; fo[v] = 1; fvec.extend(fo)\n",
    "            blocks.append(gvec + fvec)\n",
    "        while len(blocks) < self.history_len:\n",
    "            blocks.insert(0, [0]*self.dim_per_step)\n",
    "        return np.array([x for blk in blocks for x in blk], dtype=np.float32)\n",
    "\n",
    "# ---------------------------------------\n",
    "# 3) Environment + shaped reward\n",
    "# ---------------------------------------\n",
    "\n",
    "# @dataclass RewardCoeffs\n",
    "# Function: Container for all reward-shaping coefficients used by the environment.\n",
    "# Input: (constructor args; all optional with defaults)\n",
    "#   - alpha_diversity: bonus for unique letters in a guess.\n",
    "#   - beta_repeat: penalty for repeating a previously used guess.\n",
    "#   - gamma_green_sq: bonus proportional to the square of how many green positions are preserved from the previous guess.\n",
    "#   - delta_yellow: bonus for reusing previously yellow letters in different positions.\n",
    "#   - lambda_entropy: multiplier for posterior-entropy reduction (from entropy_start_turn onward).\n",
    "#   - per_step_cost: per-guess cost (penalty).\n",
    "#   - win_bonus: base win bonus (time-shaped).\n",
    "#   - win_shape_k: exponent for early-win shaping (larger → heavier early-win bias).\n",
    "#   - fail_penalty_base: baseline failure penalty at timeout.\n",
    "#   - fail_penalty_scale: scales failure penalty with fraction of turns used.\n",
    "#   - terminal_bonus / terminal_decay: legacy (unused by current win path; retained for compatibility).\n",
    "#   - entropy_start_turn: turn index at/after which entropy shaping activates.\n",
    "# Output: RewardCoeffs instance (no behavior).\n",
    "@dataclass\n",
    "class RewardCoeffs:\n",
    "    alpha_diversity: float = 0.5\n",
    "    beta_repeat: float    = 2.0\n",
    "    gamma_green_sq: float = 1.0\n",
    "    delta_yellow: float   = 0.5\n",
    "    lambda_entropy: float = 0.5\n",
    "    per_step_cost: float  = 2.0\n",
    "\n",
    "    # Nonlinear early-win shaping\n",
    "    win_bonus: float    = 100.0\n",
    "    win_shape_k: float  = 2.5   # 2–3 makes early wins much bigger\n",
    "\n",
    "    # Failure shaping\n",
    "    fail_penalty_base: float  = 20.0\n",
    "    fail_penalty_scale: float = 20.0\n",
    "\n",
    "    # legacy (unused by win now, keep for compatibility if referenced elsewhere)\n",
    "    terminal_bonus: float = 100.0\n",
    "    terminal_decay: float = 15.0\n",
    "\n",
    "    entropy_start_turn: int = 3\n",
    "    \n",
    "# class WordleEnv\n",
    "# Function: Initialize the Wordle RL environment (vocab, reward shaping, masking mode, horizon).\n",
    "# Input:\n",
    "#   - vocab: list[str] — all legal 5-letter words (baseline action space).\n",
    "#   - coeffs: RewardCoeffs — reward-shaping parameters.\n",
    "#   - mask_mode: \"hard\" (filter to posterior-consistent actions) or \"valid\" (allow all).\n",
    "#   - max_turns: int — episode horizon (typically 6).\n",
    "# Output: none (side effects: normalize vocab, build index, create StateEncoder(5), and reset()).\n",
    "class WordleEnv:\n",
    "    def __init__(self, vocab: List[str], coeffs: RewardCoeffs, mask_mode: str = \"hard\", max_turns: int = 6):\n",
    "        # mask_mode: \"hard\" = posterior-consistent only; \"valid\" = no filtering (all vocab allowed)\n",
    "        assert mask_mode in (\"valid\",\"hard\")\n",
    "        self.vocab = sorted(set([w.lower() for w in vocab if len(w)==5 and w.isalpha()]))\n",
    "        self.vocab_index = {w:i for i,w in enumerate(self.vocab)}\n",
    "        self.coeffs = coeffs\n",
    "        self.mask_mode = mask_mode\n",
    "        self.max_turns = max_turns\n",
    "        self.encoder = StateEncoder(history_len=5)  # NOTE: encodes only last 5 steps; max_turns is 6.\n",
    "        self.reset(target=random.choice(self.vocab))\n",
    "        \n",
    "    # Function: Start a new episode (optionally with a fixed target).\n",
    "    # Input: target (Optional[str]) — if provided, use this exact target; else sample from vocab.\n",
    "    # Output: (obs, mask)\n",
    "    #   - obs: torch.Tensor — encoded state, shape (145*history_len,), dtype float32.\n",
    "    #   - mask: torch.Tensor — legal-action mask, shape (len(vocab),), dtype float32.\n",
    "    def reset(self, target: Optional[str]=None):\n",
    "        self.history: List[Tuple[str, Tuple[int,...]]] = []\n",
    "        self.used: set = set()\n",
    "        self.turn = 0\n",
    "        self.done = False\n",
    "        self.target = target if target is not None else random.choice(self.vocab)\n",
    "        self.prev_posterior = self.vocab[:]\n",
    "        self.prev_entropy = math.log2(len(self.prev_posterior))\n",
    "        self._cached_post = self.vocab[:]  # cache for mask\n",
    "        return self._obs(), self.legal_action_mask()\n",
    "        \n",
    "    # Function: Internal helper — encode current history via StateEncoder.\n",
    "    # Input: none (uses self.history).\n",
    "    # Output: torch.Tensor of shape (145 * history_len,), dtype float32.\n",
    "    def _obs(self) -> torch.Tensor:\n",
    "        return torch.from_numpy(self.encoder.encode(self.history))\n",
    "        \n",
    "    # Function: Compute the current action-availability mask.\n",
    "    # Input: none (uses self.history, self._cached_post, mask_mode).\n",
    "    # Output: torch.Tensor of shape (len(self.vocab),), dtype float32.\n",
    "    #   - \"valid\": all ones.\n",
    "    #   - \"hard\": ones for posterior-consistent words, zeros otherwise.\n",
    "    #   - empty posterior (edge case): all ones (fail-open).\n",
    "    def legal_action_mask(self) -> torch.Tensor:\n",
    "        mask = np.zeros(len(self.vocab), dtype=np.float32)\n",
    "        if self.mask_mode == \"valid\":\n",
    "            mask[:] = 1.0\n",
    "        else:\n",
    "            cons = getattr(self, \"_cached_post\", None)\n",
    "            if cons is None:\n",
    "                cons = posterior_candidates(self.vocab, self.history)\n",
    "            if not cons:\n",
    "                mask[:] = 1.0\n",
    "            else:\n",
    "                idxs = [self.vocab_index[w] for w in cons]\n",
    "                mask[idxs] = 1.0\n",
    "        return torch.from_numpy(mask)\n",
    "\n",
    "    # Function: Advance one step with a guessed word; compute shaped reward; return transition.\n",
    "    # Input: guess (str) — proposed 5-letter word (must be in vocab or episode ends invalid).\n",
    "    # Output: (obs, reward, done, mask, info)\n",
    "    #   - obs: torch.Tensor — new observation after applying the guess.\n",
    "    #   - reward: float — shaped reward (per-step cost, diversity, repeat penalty, green^2, yellow reuse, entropy reduction, win bonus, fail penalty).\n",
    "    #   - done: bool — True if solved or horizon reached; else False.\n",
    "    #   - mask: torch.Tensor — updated legal action mask.\n",
    "    #   - info: (guess, feedback_tuple) on normal steps; (guess, (\"invalid\",)) if invalid guess ended the episode.\n",
    "    def step(self, guess: str):\n",
    "        if self.done:\n",
    "            raise RuntimeError(\"Episode done; call reset().\")\n",
    "        if guess not in self.vocab:\n",
    "            r = -10.0; self.done = True\n",
    "            return self._obs(), r, self.done, self.legal_action_mask(), (guess, (\"invalid\",))\n",
    "\n",
    "        self.turn += 1\n",
    "        fb = wordle_feedback(guess, self.target)\n",
    "        self.history.append((guess, fb))\n",
    "\n",
    "        c = self.coeffs\n",
    "        reward = 0.0\n",
    "        reward -= c.per_step_cost\n",
    "        reward += c.alpha_diversity * len(set(guess))\n",
    "        if guess in self.used:\n",
    "            reward -= c.beta_repeat\n",
    "        self.used.add(guess)\n",
    "\n",
    "        if len(self.history) >= 2:\n",
    "            prev_guess, prev_fb = self.history[-2]\n",
    "            greens_prev = [i for i,v in enumerate(prev_fb) if v==2]\n",
    "            matched = sum(1 for i in greens_prev if guess[i] == prev_guess[i])\n",
    "            reward += c.gamma_green_sq * (matched**2)\n",
    "            yellows_prev = [(i, prev_guess[i]) for i,v in enumerate(prev_fb) if v==1]\n",
    "            reuse = 0\n",
    "            for i,ch in yellows_prev:\n",
    "                if ch in guess and guess[i] != ch:\n",
    "                    reuse += 1\n",
    "            reward += c.delta_yellow * reuse\n",
    "\n",
    "        post = posterior_candidates(self.vocab, self.history)\n",
    "        self._cached_post = post  # cache once per step\n",
    "        H = math.log2(len(post)) if len(post) > 0 else 0.0\n",
    "        if self.turn >= c.entropy_start_turn:\n",
    "            reward += c.lambda_entropy * (self.prev_entropy - H)\n",
    "        self.prev_entropy = H\n",
    "        self.prev_posterior = post\n",
    "        \n",
    "        if guess == self.target:\n",
    "            # Nonlinear time-shaped bonus\n",
    "            rem  = self.max_turns - self.turn + 1\n",
    "            frac = rem / self.max_turns  # (0,1]\n",
    "            shaped = c.win_bonus * (frac ** c.win_shape_k)\n",
    "            reward += shaped\n",
    "            self.done = True\n",
    "        elif self.turn >= self.max_turns:\n",
    "            frac_used = min(1.0, self.turn / self.max_turns)\n",
    "            fail_pen = c.fail_penalty_base + c.fail_penalty_scale * frac_used\n",
    "            reward -= fail_pen\n",
    "            self.done = True\n",
    "\n",
    "        return self._obs(), reward, self.done, self.legal_action_mask(), (guess, fb)\n",
    "\n",
    "# --------------------------------\n",
    "# 4) Policy net (masked) + utilities\n",
    "# --------------------------------\n",
    "\n",
    "# class PolicyNet\n",
    "# Function: Feedforward policy producing raw logits over the vocabulary; helper to apply masked softmax.\n",
    "# Input (ctor): state_dim (int), vocab_size (int), hidden (tuple[int,int]) — MLP sizes.\n",
    "# Output: forward(state)-> logits (torch.Tensor shape [*, vocab_size]).\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, state_dim: int, vocab_size: int, hidden=(512,256)):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden[0]), nn.ReLU(),\n",
    "            nn.Linear(hidden[0], hidden[1]), nn.ReLU(),\n",
    "            nn.Linear(hidden[1], vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        return self.backbone(state)  # raw logits (unmasked)\n",
    "\n",
    "    # Function: Convert logits to probabilities with action masking and temperature.\n",
    "    # Input: raw_logits (Tensor [B,V]), action_mask (Tensor [B,V] with {0,1}), temperature (float).\n",
    "    # Output: (probs, invalid_mass_pre)\n",
    "    #   - probs: Tensor [B,V] — normalized over valid actions; falls back to uniform over valid if all-masked.\n",
    "    #   - invalid_mass_pre: Tensor [B] — pre-mask probability mass assigned to invalid actions (diagnostic).\n",
    "    @staticmethod\n",
    "    def masked_probs_from_logits(raw_logits: torch.Tensor, action_mask: torch.Tensor, temperature: float=1.0):\n",
    "        t = max(float(temperature), 1e-6)\n",
    "        logits = raw_logits / t\n",
    "        pre_probs = torch.softmax(logits, dim=-1)\n",
    "        pre_probs = torch.nan_to_num(pre_probs, nan=0.0)\n",
    "\n",
    "        invalid = (action_mask <= 0)\n",
    "        masked_logits = logits.masked_fill(invalid, -1e9)\n",
    "        masked_logits = torch.nan_to_num(masked_logits, nan=0.0, posinf=1e9, neginf=-1e9)\n",
    "        probs = torch.softmax(masked_logits, dim=-1)\n",
    "        probs = torch.nan_to_num(probs, nan=0.0)\n",
    "        psum = probs.sum(dim=-1, keepdim=True)\n",
    "        valid = (action_mask > 0).float()\n",
    "        valid_count = valid.sum(dim=-1, keepdim=True).clamp_min(1.0)\n",
    "        safe_uniform = valid / valid_count\n",
    "        probs = torch.where(psum > 0, probs / psum, safe_uniform)\n",
    "\n",
    "        invalid_mass_pre = (pre_probs * invalid.float()).sum(dim=-1).detach()\n",
    "        return probs, invalid_mass_pre\n",
    "\n",
    "# class ValueNet\n",
    "# Function: Feedforward value function estimator V(s).\n",
    "# Input (ctor): state_dim (int), hidden (tuple[int,int]).\n",
    "# Output: forward(state)-> value (Tensor [*]) — scalar per state.\n",
    "class ValueNet(nn.Module):\n",
    "    def __init__(self, state_dim: int, hidden=(256,128)):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden[0]), nn.ReLU(),\n",
    "            nn.Linear(hidden[0], hidden[1]), nn.ReLU(),\n",
    "            nn.Linear(hidden[1], 1)\n",
    "        )\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(state).squeeze(-1)\n",
    "\n",
    "# --------------------------------\n",
    "# 5) PPO Config (SAFER DEFAULTS + NEW KNOBS)\n",
    "# --------------------------------\n",
    "\n",
    "# @dataclass PPOConfig\n",
    "# Function: Hyperparameter bundle for PPO training (LRs, clipping, KL control, annealing, etc.).\n",
    "# Input: (all fields optional; defaults chosen for stability).\n",
    "# Output: PPOConfig instance (no behavior).\n",
    "@dataclass\n",
    "class PPOConfig:\n",
    "    # Learning rates & regularization\n",
    "    actor_lr: float = 2e-4\n",
    "    critic_lr: float = 1e-4\n",
    "    weight_decay: float = 5e-5\n",
    "\n",
    "    # Exploration controls\n",
    "    entropy_coef_start: float = 0.05\n",
    "    entropy_coef_end: float   = 0.005  # keep nonzero floor\n",
    "    temperature: float        = 1.2\n",
    "\n",
    "    # Sample regime / batch sizing\n",
    "    total_episodes: int     = 30000\n",
    "    episodes_per_batch: int = 640\n",
    "    ppo_epochs: int         = 3\n",
    "    minibatch_size: int     = 512\n",
    "\n",
    "    # Credit assignment\n",
    "    gamma: float      = 0.99\n",
    "    gae_lambda: float = 0.95\n",
    "\n",
    "    # Trust region / clipping\n",
    "    clip_eps: float       = 0.12\n",
    "    value_clip_eps: float = 0.2\n",
    "    max_grad_norm: float  = 0.5\n",
    "\n",
    "    # KL targeting & adaptation\n",
    "    kl_target: float   = 0.008\n",
    "    kl_adapt: bool     = True\n",
    "    kl_high: float     = 1.25\n",
    "    kl_low: float      = 0.95\n",
    "    actor_lr_min: float = 5e-6\n",
    "    actor_lr_max: float = 8e-4\n",
    "\n",
    "    # Eval / early stop\n",
    "    eval_every: int          = 20\n",
    "    eval_repeats: int        = 5\n",
    "    early_stop_patience: int = 40\n",
    "    min_delta: float         = 0.00\n",
    "\n",
    "    # New safety knobs\n",
    "    normalize_adv: bool   = True   # normalize advantages by default\n",
    "    clip_value_loss: bool = True   # always use value clipping\n",
    "    anneal_clip: bool     = False  # linearly decay clip_eps during training\n",
    "    entropy_decay: bool   = True   # linearly decay entropy coef\n",
    "\n",
    "    # KL penalty extras (optional, alongside clip)\n",
    "    use_kl_penalty: bool = True\n",
    "    kl_coef_start: float = 0.0\n",
    "    kl_coef_max: float   = 0.3\n",
    "\n",
    "    # Reward clipping (tame shaped reward spikes)\n",
    "    reward_clip_low: float  = -20.0\n",
    "    reward_clip_high: float = 120.0\n",
    "\n",
    "    # TRPO-style safety brake\n",
    "    trpo_brake: bool      = True\n",
    "    trpo_brake_mult: float = 2.5   # trigger if KL > mult × kl_target on a minibatch\n",
    "\n",
    "    # Logging switches\n",
    "    log_value_loss: bool = True\n",
    "    log_policy_loss: bool = True\n",
    "\n",
    "# --------------------------------\n",
    "# 6) PPO Agent\n",
    "# --------------------------------\n",
    "\n",
    "# class PPOAgent\n",
    "# Function: Tie env+models+optimizers together; collect rollouts; run PPO updates; evaluate.\n",
    "# Input (ctor): env (WordleEnv), cfg (PPOConfig).\n",
    "# Output: instance with .train() method returning history of mean returns.\n",
    "class PPOAgent:\n",
    "    def __init__(self, env: WordleEnv, cfg: PPOConfig):\n",
    "        self.env = env\n",
    "        self.cfg = cfg\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "        self.actor  = PolicyNet(env.encoder.state_dim, len(env.vocab)).to(self.device)\n",
    "        self.critic = ValueNet(env.encoder.state_dim).to(self.device)\n",
    "\n",
    "        self.opt_actor  = optim.Adam(self.actor.parameters(),  lr=cfg.actor_lr,  weight_decay=cfg.weight_decay)\n",
    "        self.opt_critic = optim.Adam(self.critic.parameters(), lr=cfg.critic_lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "        self.best_sr = -1.0\n",
    "        self.eval_no_improve = 0\n",
    "        self.lr_sched_actor  = optim.lr_scheduler.ReduceLROnPlateau(self.opt_actor,  mode='max', factor=0.5, patience=999, verbose=True)\n",
    "        self.lr_sched_critic = optim.lr_scheduler.ReduceLROnPlateau(self.opt_critic, mode='max', factor=0.5, patience=999, verbose=True)\n",
    "\n",
    "        # adaptive KL penalty weight\n",
    "        self.kl_beta = cfg.kl_coef_start\n",
    "\n",
    "    # Function: Compute GAE advantages and returns for a single episode trajectory.\n",
    "    # Input: rewards (list[float]), values (list[float]), gamma (float), lam (float).\n",
    "    # Output: (advantages: np.ndarray[T], returns: np.ndarray[T]).\n",
    "    @staticmethod\n",
    "    def _compute_gae(rewards, values, gamma, lam):\n",
    "        T = len(rewards)\n",
    "        adv = np.zeros(T, dtype=np.float32)\n",
    "        last_gae = 0.0\n",
    "        for t in reversed(range(T)):\n",
    "            v_t = values[t]\n",
    "            v_next = 0.0 if t == T-1 else values[t+1]\n",
    "            delta = rewards[t] + gamma * v_next - v_t\n",
    "            last_gae = delta + gamma * lam * last_gae\n",
    "            adv[t] = last_gae\n",
    "        returns = adv + np.array(values, dtype=np.float32)\n",
    "        return adv, returns\n",
    "\n",
    "    # Function: Collect one complete episode (sampling a target unless provided).\n",
    "    # Input: target (Optional[str]) — if given, force this target for the episode.\n",
    "    # Output: traj (list[dict]) — per-step dicts with state, mask, action, logp, value, reward, invalid_mass.\n",
    "    def _collect_episode(self, target: Optional[str] = None):\n",
    "        s, mask = self.env.reset(target=target)\n",
    "        done = False\n",
    "        traj = []\n",
    "        while not done:\n",
    "            state_b = s.unsqueeze(0).float().to(self.device)\n",
    "            mask_b  = mask.unsqueeze(0).float().to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                raw_logits = self.actor(state_b)\n",
    "                probs, invalid_mass = PolicyNet.masked_probs_from_logits(raw_logits, mask_b, self.cfg.temperature)\n",
    "                a_idx = torch.multinomial(probs[0], 1).item()\n",
    "                logp  = torch.log(probs[0, a_idx] + 1e-12).item()\n",
    "                v     = self.critic(state_b)[0].item()\n",
    "\n",
    "            s2, r, done, mask2, info = self.env.step(self.env.vocab[a_idx])\n",
    "\n",
    "            traj.append({\n",
    "                \"s\": s.numpy(),\n",
    "                \"mask\": mask.numpy(),\n",
    "                \"a\": a_idx,\n",
    "                \"logp\": float(logp),\n",
    "                \"v\": float(v),\n",
    "                \"r\": float(r),\n",
    "                \"invalid_mass\": float(invalid_mass[0].item()),\n",
    "            })\n",
    "            s, mask = s2, mask2\n",
    "        return traj\n",
    "\n",
    "    # Function: Collect a batch of episodes, flatten to tensors, and compute per-episode GAE.\n",
    "    # Input: train_targets (list[str]) — sample targets from this set for each episode.\n",
    "    # Output: batch (dict[str, torch.Tensor | int]) — tensors for PPO update.\n",
    "    def _collect_batch(self, train_targets: List[str]):\n",
    "        episodes = []\n",
    "        while len(episodes) < self.cfg.episodes_per_batch:\n",
    "            target = random.choice(train_targets)\n",
    "            ep = self._collect_episode(target)\n",
    "            episodes.append(ep)\n",
    "        # Flatten\n",
    "        states = np.concatenate([np.stack([t[\"s\"] for t in ep], axis=0) for ep in episodes], axis=0)\n",
    "        masks  = np.concatenate([np.stack([t[\"mask\"] for t in ep], axis=0) for ep in episodes], axis=0)\n",
    "        actions= np.concatenate([[t[\"a\"] for t in ep] for ep in episodes], axis=0)\n",
    "        logps  = np.concatenate([[t[\"logp\"] for t in ep] for ep in episodes], axis=0)\n",
    "        values = np.concatenate([[t[\"v\"] for t in ep] for ep in episodes], axis=0)\n",
    "        invmasses = np.concatenate([[t[\"invalid_mass\"] for t in ep] for ep in episodes], axis=0)\n",
    "\n",
    "        # GAE per-episode with reward clipping (bounded to tame spikes).\n",
    "        adv_list, ret_list = [] , []\n",
    "        for ep in episodes:\n",
    "            r_ep = [t[\"r\"] for t in ep]\n",
    "            if self.cfg.reward_clip_low is not None and self.cfg.reward_clip_high is not None:\n",
    "                r_ep = np.clip(r_ep, self.cfg.reward_clip_low, self.cfg.reward_clip_high).tolist()\n",
    "            v_ep = [t[\"v\"] for t in ep]\n",
    "            adv_ep, ret_ep = self._compute_gae(r_ep, v_ep, self.cfg.gamma, self.cfg.gae_lambda)\n",
    "            adv_list.append(adv_ep)\n",
    "            ret_list.append(ret_ep)\n",
    "        advantages = np.concatenate(adv_list, axis=0)\n",
    "        returns    = np.concatenate(ret_list, axis=0)\n",
    "\n",
    "        batch = {\n",
    "            \"states\": torch.tensor(states, dtype=torch.float32, device=self.device),\n",
    "            \"masks\":  torch.tensor(masks,  dtype=torch.float32, device=self.device),\n",
    "            \"actions\":torch.tensor(actions, dtype=torch.long, device=self.device),\n",
    "            \"logp_old\":torch.tensor(logps,  dtype=torch.float32, device=self.device),\n",
    "            \"values_old\":torch.tensor(values, dtype=torch.float32, device=self.device),\n",
    "            \"advantages\":torch.tensor(advantages, dtype=torch.float32, device=self.device),\n",
    "            \"returns\":torch.tensor(returns, dtype=torch.float32, device=self.device),\n",
    "            \"invalid_mass\":torch.tensor(invmasses, dtype=torch.float32, device=self.device),\n",
    "            \"num_episodes\": len(episodes),\n",
    "            \"num_steps\": states.shape[0],\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "    # Function: One PPO update over the provided batch (multi-epoch, mini-batches).\n",
    "    # Input: batch (dict tensors), entropy_coef (float), clip_eps_curr (float; may be tightened).\n",
    "    # Output: (mean_kl, mean_entropy, mean_policy_loss, mean_value_loss, clip_used)\n",
    "    def _ppo_update(self, batch, entropy_coef: float, clip_eps_curr: float):\n",
    "        adv = batch[\"advantages\"]\n",
    "        if self.cfg.normalize_adv:\n",
    "            std = adv.std(unbiased=False)\n",
    "            if torch.isnan(std) or std < 1e-8:\n",
    "                std = adv.new_tensor(1.0)\n",
    "            adv = (adv - adv.mean()) / std\n",
    "            adv = adv.clamp_(-10.0, 10.0)\n",
    "\n",
    "        states = batch[\"states\"]; masks = batch[\"masks\"]; actions = batch[\"actions\"]\n",
    "        logp_old = batch[\"logp_old\"]; values_old = batch[\"values_old\"]\n",
    "        returns  = batch[\"returns\"]\n",
    "\n",
    "        N = states.size(0)\n",
    "        idxs = torch.randperm(N, device=self.device)\n",
    "        mb = self.cfg.minibatch_size\n",
    "\n",
    "        with torch.no_grad():\n",
    "            old_raw_logits = self.actor(states).detach()\n",
    "            old_probs_all, _ = PolicyNet.masked_probs_from_logits(old_raw_logits, masks, temperature=self.cfg.temperature)\n",
    "\n",
    "        approx_kl_list, entropy_list = [], []\n",
    "        pol_loss_list, val_loss_list = [], []\n",
    "\n",
    "        for _ in range(self.cfg.ppo_epochs):\n",
    "            for start in range(0, N, mb):\n",
    "                mb_idx = idxs[start:start+mb]\n",
    "                s_mb = states[mb_idx]\n",
    "                m_mb = masks[mb_idx]\n",
    "                a_mb = actions[mb_idx]\n",
    "                logp_old_mb = logp_old[mb_idx]\n",
    "                v_old_mb = values_old[mb_idx]\n",
    "                ret_mb = returns[mb_idx]\n",
    "                adv_mb = adv[mb_idx]\n",
    "\n",
    "                raw_logits = self.actor(s_mb)\n",
    "                probs, _ = PolicyNet.masked_probs_from_logits(raw_logits, m_mb, temperature=self.cfg.temperature)\n",
    "                logp = torch.log(probs.gather(1, a_mb.unsqueeze(1)).squeeze(1) + 1e-12)\n",
    "\n",
    "                # --- KL for logging/brake (no grad) ---\n",
    "                with torch.no_grad():\n",
    "                    pre_ng = probs + 1e-12\n",
    "                    old_ng = old_probs_all[mb_idx] + 1e-12\n",
    "                    kl_vec_ng = (old_ng * (torch.log(old_ng) - torch.log(pre_ng))).sum(dim=1)\n",
    "                    kl_now = kl_vec_ng.mean().item()\n",
    "                    approx_kl_list.append(kl_now)\n",
    "\n",
    "                    # TRPO-style safety brake: skip actor update if minibatch jumps too far\n",
    "                    brake = False\n",
    "                    if self.cfg.trpo_brake and kl_now > self.cfg.trpo_brake_mult * self.cfg.kl_target:\n",
    "                        brake = True\n",
    "                        # 1) Immediate LR cut (actor)\n",
    "                        if self.cfg.kl_adapt:\n",
    "                            a_lr = self.opt_actor.param_groups[0]['lr']\n",
    "                            a_lr = max(self.cfg.actor_lr_min, a_lr * 0.5)   # stronger halving\n",
    "                            self.opt_actor.param_groups[0]['lr'] = a_lr\n",
    "                        # 2) Make KL penalty actually matter\n",
    "                        self.kl_beta = min(self.cfg.kl_coef_max, max(self.kl_beta, 0.05) * 2.0)\n",
    "                        # 3) Tighten the clip for the rest of THIS epoch (local cooldown)\n",
    "                        clip_eps_curr = max(0.08, clip_eps_curr * 0.75)\n",
    "                        # 4) Optional: remember we braked this epoch (to ease back later)\n",
    "                        if not hasattr(self, \"_brake_hits\"):\n",
    "                            self._brake_hits = 0\n",
    "                        self._brake_hits += 1\n",
    "                        print(f\"[Brake] minibatch KL={kl_now:.5f} > {self.cfg.trpo_brake_mult:.2f}×target; \"\n",
    "                              f\"skip actor; lrA={self.opt_actor.param_groups[0]['lr']:.2e} β_KL={self.kl_beta:.3f} \"\n",
    "                              f\"clip={clip_eps_curr:.3f}\")\n",
    "\n",
    "                # --- PPO clipped surrogate ---\n",
    "                ratio = torch.exp(logp - logp_old_mb)\n",
    "                surr1 = ratio * adv_mb\n",
    "                surr2 = torch.clamp(ratio, 1.0 - clip_eps_curr, 1.0 + clip_eps_curr) * adv_mb\n",
    "                clipped_obj = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "                # --- Differentiable KL penalty (old probs detached) ---\n",
    "                policy_loss = clipped_obj\n",
    "                if self.cfg.use_kl_penalty and self.kl_beta > 0.0:\n",
    "                    pre = probs + 1e-12\n",
    "                    old = (old_probs_all[mb_idx] + 1e-12).detach()\n",
    "                    kl_vec = (old * (torch.log(old) - torch.log(pre))).sum(dim=1)\n",
    "                    policy_loss = policy_loss + self.kl_beta * kl_vec.mean()\n",
    "\n",
    "                # --- Value loss (with optional clipping) ---\n",
    "                values = self.critic(s_mb)\n",
    "                if self.cfg.clip_value_loss:\n",
    "                    v_clipped = v_old_mb + (values - v_old_mb).clamp(-self.cfg.value_clip_eps, self.cfg.value_clip_eps)\n",
    "                    value_loss_unclipped = (values - ret_mb).pow(2)\n",
    "                    value_loss_clipped   = (v_clipped - ret_mb).pow(2)\n",
    "                    value_loss = 0.5 * torch.max(value_loss_unclipped, value_loss_clipped).mean()\n",
    "                else:\n",
    "                    value_loss = 0.5 * (values - ret_mb).pow(2).mean()\n",
    "\n",
    "                # --- Entropy bonus ---\n",
    "                entropy = -(probs * torch.log(probs + 1e-12)).sum(dim=1).mean()\n",
    "                entropy_list.append(entropy.item())\n",
    "\n",
    "                # --- Optimizer steps ---\n",
    "                if not brake:\n",
    "                    self.opt_actor.zero_grad()\n",
    "                    (policy_loss - entropy_coef * entropy).backward()\n",
    "                    nn.utils.clip_grad_norm_(self.actor.parameters(), self.cfg.max_grad_norm)\n",
    "                    self.opt_actor.step()\n",
    "\n",
    "                self.opt_critic.zero_grad()\n",
    "                value_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.critic.parameters(), self.cfg.max_grad_norm)\n",
    "                self.opt_critic.step()\n",
    "\n",
    "                pol_loss_list.append(policy_loss.item())\n",
    "                val_loss_list.append(value_loss.item())\n",
    "\n",
    "        mean_kl = float(np.mean(approx_kl_list)) if approx_kl_list else 0.0\n",
    "        mean_entropy = float(np.mean(entropy_list)) if entropy_list else 0.0\n",
    "        mean_pol = float(np.mean(pol_loss_list)) if pol_loss_list else 0.0\n",
    "        mean_val = float(np.mean(val_loss_list)) if val_loss_list else 0.0\n",
    "\n",
    "        # After the loops: soften any emergency tightening from the brake, slightly.\n",
    "        if hasattr(self, \"_brake_hits\") and self._brake_hits > 0:\n",
    "            self.kl_beta = max(0.0, self.kl_beta * 0.9)\n",
    "            self._brake_hits = 0\n",
    "\n",
    "        # IMPORTANT: return the clip that was actually used (after any tightening).\n",
    "        return mean_kl, mean_entropy, mean_pol, mean_val, clip_eps_curr\n",
    "\n",
    "    # Function: Main training loop — collect batches, update via PPO, evaluate, early-stop.\n",
    "    # Input: train_targets (list[str]), eval_targets (optional list[str]).\n",
    "    # Output: hist_returns (list[float]) — mean return per batch across training.\n",
    "    def train(self, train_targets: List[str], eval_targets: Optional[List[str]] = None):\n",
    "        cfg = self.cfg\n",
    "        episodes_done = 0\n",
    "        hist_returns = []\n",
    "        step_batches = 0\n",
    "\n",
    "        while episodes_done < cfg.total_episodes:\n",
    "            batch = self._collect_batch(train_targets)\n",
    "            episodes_done += batch[\"num_episodes\"]\n",
    "            hist_returns.append(batch[\"returns\"].mean().item())\n",
    "\n",
    "            # Linear entropy anneal (with floor).\n",
    "            if cfg.entropy_decay:\n",
    "                frac = min(1.0, episodes_done / cfg.total_episodes)\n",
    "                entropy_coef = cfg.entropy_coef_start + frac * (cfg.entropy_coef_end - cfg.entropy_coef_start)\n",
    "            else:\n",
    "                entropy_coef = cfg.entropy_coef_start\n",
    "\n",
    "            # Clip anneal (optional).\n",
    "            if cfg.anneal_clip:\n",
    "                frac = min(1.0, episodes_done / cfg.total_episodes)\n",
    "                clip_eps_curr = cfg.clip_eps * (1.0 - 0.5*frac)  # decay 50% over training\n",
    "                clip_eps_curr = max(0.08, clip_eps_curr)\n",
    "            else:\n",
    "                clip_eps_curr = cfg.clip_eps\n",
    "\n",
    "            mean_kl, mean_entropy, mean_pol, mean_val, clip_used = self._ppo_update(batch, entropy_coef, clip_eps_curr)\n",
    "            step_batches += 1\n",
    "\n",
    "            # Adaptive KL-based LR tweak (actor) and KL penalty weight.\n",
    "            if cfg.kl_adapt:\n",
    "                a_lr = self.opt_actor.param_groups[0]['lr']\n",
    "                hot = (mean_kl > cfg.kl_high * cfg.kl_target)\n",
    "                cold = (mean_kl < cfg.kl_low  * cfg.kl_target)\n",
    "                if hot:\n",
    "                    a_lr = max(cfg.actor_lr_min, a_lr * 0.5)\n",
    "                    self.kl_beta = min(cfg.kl_coef_max, max(self.kl_beta, 0.05) * 1.5)\n",
    "                elif cold:\n",
    "                    if mean_kl < 0.5 * cfg.kl_target:\n",
    "                        a_lr = min(cfg.actor_lr_max, a_lr * 1.8)\n",
    "                    else:\n",
    "                        a_lr = min(cfg.actor_lr_max, a_lr * 1.2)\n",
    "                    self.kl_beta = max(0.0, self.kl_beta * 0.7)\n",
    "                self.opt_actor.param_groups[0]['lr'] = a_lr\n",
    "            \n",
    "            if step_batches % 5 == 0:\n",
    "                # Controller heartbeat\n",
    "                print(f\"[KLctl] KL={mean_kl:.5f}/{cfg.kl_target:.5f}  \"\n",
    "                      f\"lrA={self.opt_actor.param_groups[0]['lr']:.2e}  \"\n",
    "                      f\"beta={self.kl_beta:.3f}\")\n",
    "            \n",
    "                # PPO summary\n",
    "                tail = hist_returns[-10:] if len(hist_returns) >= 10 else hist_returns\n",
    "                print(\n",
    "                    f\"[PPO] batches={step_batches} ep_done={episodes_done}/{cfg.total_episodes} \"\n",
    "                    f\"avg_ret_last10={np.mean(tail):.2f}  KL={mean_kl:.6f}  H={mean_entropy:.3f}  \"\n",
    "                    f\"β_KL={self.kl_beta:.3f} clip={clip_used:.3f}  \"\n",
    "                    f\"pre_mask_invalid_mass_mean={batch['invalid_mass'].mean().item():.4f}  \"\n",
    "                    + (f\"Vloss={mean_val:.3f}  \" if cfg.log_value_loss else \"\")\n",
    "                    + (f\"Ploss={mean_pol:.3f}\" if cfg.log_policy_loss else \"\")\n",
    "                )\n",
    "\n",
    "            # periodic eval\n",
    "            if step_batches == 1 or step_batches % cfg.eval_every == 0:\n",
    "                val_targets = eval_targets if eval_targets is not None else train_targets\n",
    "                sr, avg_t, ci = evaluate_greedy(self, val_targets, temperature=0.3, repeats=cfg.eval_repeats, ci=True)\n",
    "                print(f\"[Eval] SR={sr:.3f}±{ci:.3f}  avg_turns={avg_t:.2f}  lrA={self.opt_actor.param_groups[0]['lr']:.2e} lrC={self.opt_critic.param_groups[0]['lr']:.2e}\")\n",
    "                self.lr_sched_actor.step(sr)\n",
    "                self.lr_sched_critic.step(sr)\n",
    "                improved = (sr > self.best_sr + cfg.min_delta)\n",
    "                if improved:\n",
    "                    self.best_sr = sr\n",
    "                    torch.save({'actor': self.actor.state_dict(),\n",
    "                                'critic': self.critic.state_dict(),\n",
    "                                'sr': sr, 'episodes_done': episodes_done},\n",
    "                               \"ppo_wordle_best.pt\")\n",
    "                    self.eval_no_improve = 0\n",
    "                else:\n",
    "                    self.eval_no_improve += 1\n",
    "                    if self.eval_no_improve >= cfg.early_stop_patience:\n",
    "                        print(\"[EarlyStop] No val SR improvement; stopping training.\")\n",
    "                        break\n",
    "        return hist_returns\n",
    "\n",
    "# --------------------------\n",
    "# 7) Greedy evaluation (+ CI)\n",
    "# --------------------------\n",
    "\n",
    "# Function: Evaluate the trained policy greedily (argmax) over a set of targets.\n",
    "# Input: agent (PPOAgent), targets (list[str]), temperature (float), repeats (int), ci (bool).\n",
    "# Output:\n",
    "#   - if ci=True: (sr, avg_t, 95% CI width) where sr is success rate, avg_t is avg turns on successes.\n",
    "#   - else: (sr, avg_t).\n",
    "@torch.no_grad()\n",
    "def evaluate_greedy(agent, targets: List[str], temperature: float = 0.5, repeats: int = 1, ci: bool=False):\n",
    "    rng_state = random.getstate()\n",
    "    wins, turns = 0, []\n",
    "    for rep in range(repeats):\n",
    "        random.seed(1234 + rep)  # fixed seeds for stability across repeats\n",
    "        for tgt in targets:\n",
    "            s, mask = agent.env.reset(target=tgt)\n",
    "            done = False; t = 0\n",
    "            while not done:\n",
    "                t += 1\n",
    "                state = s.unsqueeze(0).float().to(agent.device)\n",
    "                mask_b = mask.unsqueeze(0).float().to(agent.device)\n",
    "                raw_logits = agent.actor(state)\n",
    "                probs, _ = PolicyNet.masked_probs_from_logits(raw_logits, mask_b, temperature=temperature)\n",
    "                a_idx = torch.argmax(probs[0]).item()\n",
    "                word = agent.env.vocab[a_idx]\n",
    "                s, r, done, mask, info = agent.env.step(word)\n",
    "            solved = (agent.env.history and agent.env.history[-1][0] == tgt)\n",
    "            if solved:\n",
    "                wins += 1; turns.append(t)\n",
    "    random.setstate(rng_state)\n",
    "    n = len(targets) * repeats if targets else 0\n",
    "    sr = wins / n if n else 0.0\n",
    "    avg_t = float(np.mean(turns)) if turns else float('inf')\n",
    "    if ci:\n",
    "        if n == 0:\n",
    "            return sr, avg_t, 0.0\n",
    "        se = math.sqrt(sr*(1-sr)/n)\n",
    "        return sr, avg_t, 1.96*se\n",
    "    return sr, avg_t\n",
    "\n",
    "# --------------------------\n",
    "# 8) Visualization helpers\n",
    "# --------------------------\n",
    "\n",
    "# Function: Convert feedback tuple to Unicode square emojis.\n",
    "# Input: fb_tuple (iterable of ints in {0,1,2}).\n",
    "# Output: str — e.g., \"⬛🟨🟩⬛⬛\".\n",
    "EMO = {0: \"⬛\", 1: \"🟨\", 2: \"🟩\"}\n",
    "\n",
    "def fb_to_squares(fb_tuple):\n",
    "    return \"\".join(EMO[int(v)] for v in fb_tuple)\n",
    "\n",
    "# Function: Roll out a greedy (or stochastic) episode on a fixed target and print step-by-step.\n",
    "# Input: agent (PPOAgent), target (str), temperature (float), greedy (bool).\n",
    "# Output: none (pretty-prints guesses, feedback, reward, and candidate count).\n",
    "@torch.no_grad()\n",
    "def visualize_episode(agent, target: str, temperature: float = 0.5, greedy: bool = True):\n",
    "    env = agent.env\n",
    "    s, mask = env.reset(target=target)\n",
    "    done, t = False, 0\n",
    "    print(f\"=== Visualize: target = {target.upper()} ===\")\n",
    "    while not done:\n",
    "        t += 1\n",
    "        state  = s.unsqueeze(0).float().to(agent.device)\n",
    "        mask_b = mask.unsqueeze(0).float().to(agent.device)\n",
    "        raw_logits = agent.actor(state)\n",
    "        probs, _  = PolicyNet.masked_probs_from_logits(raw_logits, mask_b, temperature=temperature)\n",
    "        a_idx  = torch.argmax(probs[0]).item() if greedy else torch.multinomial(probs[0], 1).item()\n",
    "        guess  = env.vocab[a_idx]\n",
    "        s, r, done, mask, info = env.step(guess)\n",
    "        _, fb = info\n",
    "        fb_str = fb_to_squares(fb)\n",
    "        cand_cnt = len(env.prev_posterior) if hasattr(env, \"prev_posterior\") else float(\"nan\")\n",
    "        print(f\"t{t}: {guess.upper():<8}  {fb_str}   r={r:6.2f}   candidates≈{cand_cnt}\")\n",
    "    solved = (env.history and env.history[-1][0] == target)\n",
    "    print(f\"Result: {'✅ SOLVED' if solved else '❌ FAILED'} in {t} turns.\")\n",
    "\n",
    "# -------------------------\n",
    "# 9) Main\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    random.seed(0); np.random.seed(0); torch.manual_seed(0)\n",
    "\n",
    "    # Function: Optional deterministic mode for debugging / reproducibility.\n",
    "    # Input: env var DETERMINISTIC=\"1\" enables cudnn deterministic algorithms.\n",
    "    # Output: none (side-effect: slower but bitwise-stable kernels where possible).\n",
    "    if os.environ.get(\"DETERMINISTIC\", \"0\") == \"1\":\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(0)\n",
    "        os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
    "\n",
    "    # Load vocab/solutions; split into train/val/test by targets.\n",
    "    vocab, solutions = load_wordle_lists(cache_dir=\".wordlists\")\n",
    "\n",
    "    random.shuffle(solutions)\n",
    "    n_sol   = len(solutions)\n",
    "    n_train = int(0.80 * n_sol)\n",
    "    n_val   = int(0.10 * n_sol)\n",
    "    n_test  = n_sol - n_train - n_val\n",
    "    train_targets = solutions[:n_train]\n",
    "    val_targets   = solutions[n_train:n_train+n_val]\n",
    "    test_targets  = solutions[n_train+n_val:n_train+n_val+n_test]\n",
    "    assert len(train_targets) + len(val_targets) + len(test_targets) == n_sol\n",
    "\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    print(f\"Split sizes -> train: {len(train_targets)}, val: {len(val_targets)}, test: {len(test_targets)}\")\n",
    "\n",
    "    # Reward shaping coefficients (tuned for stability and signal richness).\n",
    "    coeffs = RewardCoeffs(\n",
    "        alpha_diversity=0.5, beta_repeat=2.0,\n",
    "        gamma_green_sq=1.0, delta_yellow=0.5,\n",
    "        lambda_entropy=0.5, per_step_cost=2.0,\n",
    "        win_bonus=100.0, win_shape_k=2.5,\n",
    "        fail_penalty_base=20.0, fail_penalty_scale=20.0,\n",
    "        entropy_start_turn=3\n",
    "    )\n",
    "\n",
    "    # NOTE: WordleEnv currently encodes only last 5 steps (StateEncoder(5)) even though max_turns=6.\n",
    "    env = WordleEnv(vocab=vocab, coeffs=coeffs, mask_mode=\"hard\", max_turns=6)\n",
    "\n",
    "    cfg = PPOConfig()  # Safer defaults (entropy floor, KL control, value clip, etc.)\n",
    "    agent = PPOAgent(env, cfg)\n",
    "\n",
    "    print(f\"Training on {len(train_targets)} targets; validating on {len(val_targets)}; holding out {len(test_targets)} for FINAL test.\")\n",
    "    agent.train(train_targets, eval_targets=val_targets)\n",
    "\n",
    "    # === Final TEST (touch ONCE) ===\n",
    "    # Function: Load best checkpoint (if present) and evaluate on held-out test set.\n",
    "    # Input: ckpt_path (str) — \"ppo_wordle_best.pt\" saved during training.\n",
    "    # Output: prints test SR ± CI and avg turns; no state is persisted beyond printed results.\n",
    "    ckpt_path = \"ppo_wordle_best.pt\"\n",
    "    if os.path.exists(ckpt_path):\n",
    "        ckpt = torch.load(ckpt_path, map_location=agent.device)\n",
    "        agent.actor.load_state_dict(ckpt['actor'])\n",
    "        agent.critic.load_state_dict(ckpt['critic'])\n",
    "        print(f\"Loaded best checkpoint: episodes_done={ckpt.get('episodes_done','?')}, val_SR={ckpt.get('sr','?'):.3f}\")\n",
    "\n",
    "    sr, avg_t, ci = evaluate_greedy(agent, test_targets, temperature=0.3, repeats=1, ci=True)\n",
    "    print(f\"[FINAL TEST] success_rate={sr:.3f}±{ci:.3f}, avg_turns={avg_t:.2f} on {len(test_targets)} held-out targets (evaluated once)\")\n",
    "\n",
    "    print(\"Visualizing a few held-out *test* targets...\")\n",
    "    if len(test_targets) > 0:\n",
    "        for tgt in random.sample(test_targets, k=min(10, len(test_targets))):\n",
    "            visualize_episode(agent, target=tgt, temperature=0.5, greedy=True)\n",
    "        if \"crane\" in env.vocab:\n",
    "            visualize_episode(agent, target=\"crane\", temperature=0.5, greedy=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
